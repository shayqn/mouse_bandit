{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "This logistic regression is based on the Beeler/Daw et al. 2010 paper.\n",
    "Specifically:\n",
    "\n",
    "dependent variable: binary choice of port (-1 or 1)\n",
    "\n",
    "explanatory variables: \n",
    "1. the N previous rewards $ r_{t-N:t-1} $\n",
    "2. the previous choice $c_{t-1}$ to capture a tendency to stay or switch\n",
    "3. bias variable (1) to capture fixed, overall preference for either port\n",
    "\n",
    "Note: this model only carries information about ports when it gets a reward. IE -1 = right reward, 1 = left reward, but 0 = no reward (for either side). Should compare models with this information vs. added information about the non-rewarded port choices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/shayneufeld/GitHub/mouse_bandit/data_preprocessing_code')\n",
    "sys.path.append('/Users/shayneufeld/GitHub/mouse_bandit')\n",
    "import support_functions as sf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import bandit_preprocessing as bp\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define function to do logistic regression and some basic evalutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "I compiled the code above into a more compact function so I can cycle through different conditions/mice/etc \n",
    "as neccessary\n",
    "'''\n",
    "\n",
    "def logreg_and_eval(data,num_rewards=10,test_data=False):\n",
    "    '''\n",
    "    Perform Logistic Regression on a pandas dataframe of trials (from feature matrix)\n",
    "    \n",
    "    Inputs:\n",
    "        - data: pandas dataframe of trials (from feature matrix)\n",
    "    Outputs:\n",
    "        - logreg: trained logistic regression model (from sklearn)\n",
    "        - stats:  pandas dataframe with F1, pseudo-R2, and BIC scores from model\n",
    "        - coeffs: beta coefficients from logreg\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    from statsmodels.discrete.discrete_model import Logit\n",
    "    \n",
    "    port_features = []\n",
    "    reward_features = []\n",
    "\n",
    "    #change right port to -1 instead of 0\n",
    "    for col in data:\n",
    "        if '_Port' in col:\n",
    "            data.loc[data[col] == 0,col] = -1\n",
    "            port_features.append(col)\n",
    "        elif '_Reward' in col:\n",
    "            reward_features.append(col)\n",
    "\n",
    "    #create new feature matrix\n",
    "    d = data.copy()\n",
    "    for i in range(len(port_features)):\n",
    "        d[reward_features[i]] = d[reward_features[i]].values*d[port_features[i]].values\n",
    "    \n",
    "    \n",
    "    #determine the features\n",
    "    features = reward_features.copy()\n",
    "    features = features[-1*num_rewards:] #only take the num of rewards specificied in the function\n",
    "    features.append('1_Port') #append the last decision as a feature\n",
    "    features.append('Decision') #finally append the decision so we can take it to predict later\n",
    "    \n",
    "    #final version of data\n",
    "    d = d[features].copy() #this now just has the features we want and the decision we want to predict\n",
    "    \n",
    "    #do the same thing for the test data if it exists!\n",
    "    if test_data is not False:\n",
    "        for col in test_data:\n",
    "            if '_Port' in col:\n",
    "                test_data.loc[test_data[col] == 0,col] = -1\n",
    "\n",
    "        #create new feature matrix\n",
    "        data_test_new = test_data.copy()\n",
    "        for i in range(len(port_features)):\n",
    "            data_test_new[reward_features[i]] = test_data[reward_features[i]].values*test_data[port_features[i]].values\n",
    "        \n",
    "        d_test = data_test_new[features].copy()\n",
    "    \n",
    "    \n",
    "        #set training and testing sets now\n",
    "        x_train = d.iloc[:,:-1].values\n",
    "        y_train = d.iloc[:,-1].values\n",
    "        x_test = d_test.iloc[:,:-1].values\n",
    "        y_test = d_test.iloc[:,-1].values\n",
    "        \n",
    "        prev_port_test = d_test['1_Port'].values\n",
    "        prev_port_test[prev_port_test==-1] = 0\n",
    "    \n",
    "    #if there is no test data, then split up the data into training and testing\n",
    "    else:\n",
    "        #extract features and decisions\n",
    "        x = d.iloc[:,:-1].values\n",
    "        y = d.iloc[:,-1].values\n",
    "\n",
    "        #split into training and testing\n",
    "        n_trials = x.shape[0]\n",
    "        shuf_inds = np.random.permutation(n_trials)\n",
    "        split_ind = int(n_trials*0.7)\n",
    "\n",
    "        x_train = x[shuf_inds[:split_ind],:]\n",
    "        y_train = y[shuf_inds[:split_ind]]\n",
    "\n",
    "        x_test = x[shuf_inds[split_ind:],:]\n",
    "        y_test = y[shuf_inds[split_ind:]]\n",
    "        \n",
    "        #extract previous port decision for test set\n",
    "        #these will be used to calculate switches on the test predictions\n",
    "        prev_port_test = d['1_Port'].values[shuf_inds[split_ind:]]\n",
    "        prev_port_test[prev_port_test==-1] = 0\n",
    "    \n",
    "    '''\n",
    "    Modeling\n",
    "    '''\n",
    "    \n",
    "    #fit logistic regression\n",
    "    logreg = sklearn.linear_model.LogisticRegressionCV()\n",
    "    logreg.fit(x_train,y_train)\n",
    "    \n",
    "    #predict on testing set\n",
    "    y_predict = logreg.predict(x_test)\n",
    "    y_predict_proba = logreg.predict_proba(x_test)\n",
    "    \n",
    "    #model accuracy\n",
    "    score = logreg.score(x_test,y_test)\n",
    "    \n",
    "    #calculating pseudo-R2 and BIC from statsmodel OLS\n",
    "    model = Logit(y_train,x_train)\n",
    "    rslt  = model.fit()\n",
    "\n",
    "    #switches\n",
    "    y_test_switch = np.abs(y_test - prev_port_test)\n",
    "    y_predict_switch = np.abs(y_predict - prev_port_test)\n",
    "    acc_pos,acc_neg,F1=sf.score_both_and_confuse(y_predict_switch,y_test_switch,confusion=False,disp=True)\n",
    "    \n",
    "    #extract coefficients\n",
    "    coefs = logreg.coef_ #retrieve coefs\n",
    "    coefs = np.append(coefs[0],logreg.intercept_) #add bias coef\n",
    "    \n",
    "    #create stats database to return\n",
    "    d_ = {'pseudo-R2':rslt.prsquared,'stay':acc_pos,'switch':acc_neg,'Accuracy':score,\n",
    "          'BIC':rslt.bic,'negative loglikelihood':-1*rslt.llf,'F1':F1}\n",
    "    stats = pd.DataFrame(data=d_,index=[0])\n",
    "    features = features[:-1]\n",
    "    features.append('Bias')\n",
    "    \n",
    "    coefs = pd.DataFrame(data=coefs.reshape(1,-1),columns=features)\n",
    "    return logreg,stats,coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logreg_and_eval_withports(data,num_rewards=10,num_ports=1,test_data=False):\n",
    "    '''\n",
    "    Perform Logistic Regression on a pandas dataframe of trials (from feature matrix)\n",
    "    \n",
    "    Inputs:\n",
    "        - data: pandas dataframe of trials (from feature matrix)\n",
    "    Outputs:\n",
    "        - logreg: trained logistic regression model (from sklearn)\n",
    "        - stats:  pandas dataframe with F1, pseudo-R2, and BIC scores from model\n",
    "        - coeffs: beta coefficients from logreg\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    from statsmodels.discrete.discrete_model import Logit\n",
    "    \n",
    "    port_features = []\n",
    "    reward_features = []\n",
    "\n",
    "    #change right port to -1 instead of 0\n",
    "    for col in data:\n",
    "        if '_Port' in col:\n",
    "            data.loc[data[col] == 0,col] = -1\n",
    "            port_features.append(col)\n",
    "        elif '_Reward' in col:\n",
    "            reward_features.append(col)\n",
    "\n",
    "    #create new feature matrix\n",
    "    d = data.copy()\n",
    "    for i in range(len(port_features)):\n",
    "        d[reward_features[i]] = d[reward_features[i]].values*d[port_features[i]].values\n",
    "    \n",
    "    \n",
    "    #determine the features\n",
    "    features = reward_features.copy()\n",
    "    if num_rewards == 0:\n",
    "        features = port_features[-1*num_ports:]\n",
    "    elif num_ports == 0:\n",
    "        features = features[-1*num_rewards:] #only take the num of rewards specificied in the function\n",
    "    else:\n",
    "        features = features[-1*num_rewards:]\n",
    "        features = np.append(features,port_features[-1*num_ports:])\n",
    "    \n",
    "    print(features)\n",
    "    features = np.append(features,'Decision') #finally append the decision so we can take it to predict later\n",
    "    \n",
    "    \n",
    "    \n",
    "    #final version of data\n",
    "    d = d[features].copy() #this now just has the features we want and the decision we want to predict\n",
    "    \n",
    "    #do the same thing for the test data if it exists!\n",
    "    if test_data is not False:\n",
    "        for col in test_data:\n",
    "            if '_Port' in col:\n",
    "                test_data.loc[test_data[col] == 0,col] = -1\n",
    "\n",
    "        #create new feature matrix\n",
    "        data_test_new = test_data.copy()\n",
    "        for i in range(len(port_features)):\n",
    "            data_test_new[reward_features[i]] = test_data[reward_features[i]].values*test_data[port_features[i]].values\n",
    "        \n",
    "        d_test = data_test_new[features].copy()\n",
    "    \n",
    "    \n",
    "        #set training and testing sets now\n",
    "        x_train = d.iloc[:,:-1].values\n",
    "        y_train = d.iloc[:,-1].values\n",
    "        x_test = d_test.iloc[:,:-1].values\n",
    "        y_test = d_test.iloc[:,-1].values\n",
    "        \n",
    "        prev_port_test = test_data['1_Port'].values\n",
    "        prev_port_test[prev_port_test==-1] = 0\n",
    "    \n",
    "    #if there is no test data, then split up the data into training and testing\n",
    "    else:\n",
    "        #extract features and decisions\n",
    "        x = d.iloc[:,:-1].values\n",
    "        y = d.iloc[:,-1].values\n",
    "\n",
    "        #split into training and testing\n",
    "        n_trials = x.shape[0]\n",
    "        shuf_inds = np.random.permutation(n_trials)\n",
    "        split_ind = int(n_trials*0.7)\n",
    "\n",
    "        x_train = x[shuf_inds[:split_ind],:]\n",
    "        y_train = y[shuf_inds[:split_ind]]\n",
    "\n",
    "        x_test = x[shuf_inds[split_ind:],:]\n",
    "        y_test = y[shuf_inds[split_ind:]]\n",
    "        \n",
    "        #extract previous port decision for test set\n",
    "        #these will be used to calculate switches on the test predictions\n",
    "        prev_port_test = data['1_Port'].values[shuf_inds[split_ind:]]\n",
    "        prev_port_test[prev_port_test==-1] = 0\n",
    "    \n",
    "    '''\n",
    "    Modeling\n",
    "    '''\n",
    "    \n",
    "    #fit logistic regression\n",
    "    logreg = sklearn.linear_model.LogisticRegressionCV()\n",
    "    logreg.fit(x_train,y_train)\n",
    "    \n",
    "    #predict on testing set\n",
    "    y_predict = logreg.predict(x_test)\n",
    "    y_predict_proba = logreg.predict_proba(x_test)\n",
    "    \n",
    "    #model accuracy\n",
    "    score = logreg.score(x_test,y_test)\n",
    "    \n",
    "    #calculating pseudo-R2 and BIC from statsmodel OLS\n",
    "    model = Logit(y_train,x_train)\n",
    "    rslt  = model.fit()\n",
    "\n",
    "    #switches\n",
    "    y_test_switch = np.abs(y_test - prev_port_test)\n",
    "    y_predict_switch = np.abs(y_predict - prev_port_test)\n",
    "    acc_pos,acc_neg,F1=sf.score_both_and_confuse(y_predict_switch,y_test_switch,confusion=False,disp=True)\n",
    "    \n",
    "    #extract coefficients\n",
    "    coefs = logreg.coef_ #retrieve coefs\n",
    "    coefs = np.append(coefs[0],logreg.intercept_) #add bias coef\n",
    "    \n",
    "    #create stats database to return\n",
    "    d_ = {'pseudo-R2':rslt.prsquared,'stay':acc_pos,'switch':acc_neg,'Accuracy':score,\n",
    "          'BIC':rslt.bic,'negative loglikelihood':-1*rslt.llf,'F1':F1}\n",
    "    stats = pd.DataFrame(data=d_,index=[0])\n",
    "    features = features[:-1]\n",
    "    features = np.append(features,'Bias')\n",
    "    \n",
    "    coefs = pd.DataFrame(data=coefs.reshape(1,-1),columns=features)\n",
    "    return logreg,stats,coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/shayneufeld/GitHub/mouse_bandit/data/processed_data/master_data.csv',index_col=0)\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29777, 54)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['Condition']=='70-30'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/CaGRIN/lib/python3.5/site-packages/pandas/core/indexing.py:465: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.273970\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/CaGRIN/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.274399\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            1.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.274912\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            1.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.276413\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.276452\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.275314\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            1.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.273934\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            1.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.275009\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.274619\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.276097\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.273300\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            1.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.274145\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.272431\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            1.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.274687\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.274569\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.274624\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            1.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.276033\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.272862\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            1.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.271120\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            1.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.277577\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.277425\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.278539\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.273575\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.276197\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.273142\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.276207\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.274583\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.273950\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.274187\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.277234\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.306535\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.306279\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.308815\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.309327\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.310504\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.306967\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.311083\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.308491\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.308030\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.307444\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.309146\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.311453\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.305920\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.309789\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.310934\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.306579\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.307431\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.308419\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.305358\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.308643\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.308819\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.310666\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.310416\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.306948\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.309807\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.311055\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.309054\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.311008\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.312514\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.309224\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.288728\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298299\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.295582\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.291843\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.297101\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.293027\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.292443\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.296237\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.294218\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.299619\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.291359\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.288759\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.297771\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.297176\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.293616\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.292338\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.295601\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.294340\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.294377\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.297576\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.294414\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.291821\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.292499\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.289079\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.294340\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.297110\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.297671\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.289041\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298644\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.293678\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO            0.0            0.0\n",
      "True YES           0.0            0.0\n",
      "\n",
      "F1: 0.000\n",
      "\n",
      "Accuracy on class 0: nan\n",
      "Accuracy on class 1: nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conditions = ['90-10','80-20','70-30']\n",
    "\n",
    "for i,condition in enumerate(conditions):\n",
    "    d = data[data['Condition']==condition]\n",
    "    \n",
    "    for j in range(30):\n",
    "        model_curr,stats_curr,coefs_curr = logreg_and_eval(d,num_rewards=7)\n",
    "        models.append(models)\n",
    "\n",
    "        if ((i == 0 and j == 0)):\n",
    "            stats_0 = stats_curr.copy()\n",
    "            coefs_0 = coefs_curr.copy()\n",
    "        else:\n",
    "            stats_0 = stats_0.append(stats_curr)\n",
    "            coefs_0 = coefs_0.append(coefs_curr)\n",
    "\n",
    "c = np.zeros(90)+70\n",
    "c[30:] = 80\n",
    "c[60:] = 90\n",
    "\n",
    "stats_0.insert(0,'Condition',condition)\n",
    "coefs_0.insert(0,'Condition',condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>BIC</th>\n",
       "      <th>F1</th>\n",
       "      <th>negative loglikelihood</th>\n",
       "      <th>pseudo-R2</th>\n",
       "      <th>stay</th>\n",
       "      <th>switch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.917395</td>\n",
       "      <td>21443.296522</td>\n",
       "      <td>0</td>\n",
       "      <td>10679.365045</td>\n",
       "      <td>0.604736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.920328</td>\n",
       "      <td>21476.735870</td>\n",
       "      <td>0</td>\n",
       "      <td>10696.084719</td>\n",
       "      <td>0.604125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.919131</td>\n",
       "      <td>21516.728687</td>\n",
       "      <td>0</td>\n",
       "      <td>10716.081128</td>\n",
       "      <td>0.603373</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.921166</td>\n",
       "      <td>21633.695044</td>\n",
       "      <td>0</td>\n",
       "      <td>10774.564306</td>\n",
       "      <td>0.601216</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.921944</td>\n",
       "      <td>21636.795112</td>\n",
       "      <td>0</td>\n",
       "      <td>10776.114340</td>\n",
       "      <td>0.601157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.918891</td>\n",
       "      <td>21548.051488</td>\n",
       "      <td>0</td>\n",
       "      <td>10731.742528</td>\n",
       "      <td>0.602749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.918233</td>\n",
       "      <td>21440.499153</td>\n",
       "      <td>0</td>\n",
       "      <td>10677.966361</td>\n",
       "      <td>0.604790</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.920687</td>\n",
       "      <td>21524.305845</td>\n",
       "      <td>0</td>\n",
       "      <td>10719.869707</td>\n",
       "      <td>0.603222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.919370</td>\n",
       "      <td>21493.868466</td>\n",
       "      <td>0</td>\n",
       "      <td>10704.651017</td>\n",
       "      <td>0.603801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.921585</td>\n",
       "      <td>21609.055473</td>\n",
       "      <td>0</td>\n",
       "      <td>10762.244520</td>\n",
       "      <td>0.601671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.918891</td>\n",
       "      <td>21391.003669</td>\n",
       "      <td>0</td>\n",
       "      <td>10653.218619</td>\n",
       "      <td>0.605702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.920867</td>\n",
       "      <td>21456.874058</td>\n",
       "      <td>0</td>\n",
       "      <td>10686.153813</td>\n",
       "      <td>0.604493</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.918592</td>\n",
       "      <td>21323.309133</td>\n",
       "      <td>0</td>\n",
       "      <td>10619.371351</td>\n",
       "      <td>0.606952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.918472</td>\n",
       "      <td>21499.168802</td>\n",
       "      <td>0</td>\n",
       "      <td>10707.301185</td>\n",
       "      <td>0.603707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.920029</td>\n",
       "      <td>21489.962001</td>\n",
       "      <td>0</td>\n",
       "      <td>10702.697785</td>\n",
       "      <td>0.603871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.919849</td>\n",
       "      <td>21494.238795</td>\n",
       "      <td>0</td>\n",
       "      <td>10704.836182</td>\n",
       "      <td>0.603787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.920867</td>\n",
       "      <td>21604.122741</td>\n",
       "      <td>0</td>\n",
       "      <td>10759.778155</td>\n",
       "      <td>0.601765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.918712</td>\n",
       "      <td>21356.853849</td>\n",
       "      <td>0</td>\n",
       "      <td>10636.143709</td>\n",
       "      <td>0.606341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.917036</td>\n",
       "      <td>21221.120032</td>\n",
       "      <td>0</td>\n",
       "      <td>10568.276800</td>\n",
       "      <td>0.608835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.921525</td>\n",
       "      <td>21724.438519</td>\n",
       "      <td>0</td>\n",
       "      <td>10819.936043</td>\n",
       "      <td>0.599540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.923082</td>\n",
       "      <td>21712.622336</td>\n",
       "      <td>0</td>\n",
       "      <td>10814.027952</td>\n",
       "      <td>0.599756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.922363</td>\n",
       "      <td>21799.483833</td>\n",
       "      <td>0</td>\n",
       "      <td>10857.458701</td>\n",
       "      <td>0.598121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.918413</td>\n",
       "      <td>21412.498269</td>\n",
       "      <td>0</td>\n",
       "      <td>10663.965918</td>\n",
       "      <td>0.605307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.920687</td>\n",
       "      <td>21616.859048</td>\n",
       "      <td>0</td>\n",
       "      <td>10766.146308</td>\n",
       "      <td>0.601519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.917156</td>\n",
       "      <td>21378.689081</td>\n",
       "      <td>0</td>\n",
       "      <td>10647.061324</td>\n",
       "      <td>0.605933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.920388</td>\n",
       "      <td>21617.627384</td>\n",
       "      <td>0</td>\n",
       "      <td>10766.530476</td>\n",
       "      <td>0.601516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.919550</td>\n",
       "      <td>21491.020866</td>\n",
       "      <td>0</td>\n",
       "      <td>10703.227217</td>\n",
       "      <td>0.603851</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.917994</td>\n",
       "      <td>21441.684882</td>\n",
       "      <td>0</td>\n",
       "      <td>10678.559225</td>\n",
       "      <td>0.604767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.918532</td>\n",
       "      <td>21460.164965</td>\n",
       "      <td>0</td>\n",
       "      <td>10687.799267</td>\n",
       "      <td>0.604424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.921525</td>\n",
       "      <td>21697.710132</td>\n",
       "      <td>0</td>\n",
       "      <td>10806.571850</td>\n",
       "      <td>0.600031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.909671</td>\n",
       "      <td>12115.468357</td>\n",
       "      <td>0</td>\n",
       "      <td>6017.955085</td>\n",
       "      <td>0.583357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.918178</td>\n",
       "      <td>12514.467371</td>\n",
       "      <td>0</td>\n",
       "      <td>6217.454592</td>\n",
       "      <td>0.569543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.916835</td>\n",
       "      <td>12401.209511</td>\n",
       "      <td>0</td>\n",
       "      <td>6160.825662</td>\n",
       "      <td>0.573457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.913029</td>\n",
       "      <td>12245.311720</td>\n",
       "      <td>0</td>\n",
       "      <td>6082.876766</td>\n",
       "      <td>0.578831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.915603</td>\n",
       "      <td>12464.500849</td>\n",
       "      <td>0</td>\n",
       "      <td>6192.471331</td>\n",
       "      <td>0.571208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.911686</td>\n",
       "      <td>12294.668497</td>\n",
       "      <td>0</td>\n",
       "      <td>6107.555155</td>\n",
       "      <td>0.577095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.911798</td>\n",
       "      <td>12270.338363</td>\n",
       "      <td>0</td>\n",
       "      <td>6095.390088</td>\n",
       "      <td>0.577924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.916163</td>\n",
       "      <td>12428.511824</td>\n",
       "      <td>0</td>\n",
       "      <td>6174.476818</td>\n",
       "      <td>0.572446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.914260</td>\n",
       "      <td>12344.347092</td>\n",
       "      <td>0</td>\n",
       "      <td>6132.394452</td>\n",
       "      <td>0.575440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>12569.470209</td>\n",
       "      <td>0</td>\n",
       "      <td>6244.956011</td>\n",
       "      <td>0.567677</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.911686</td>\n",
       "      <td>12225.162477</td>\n",
       "      <td>0</td>\n",
       "      <td>6072.802145</td>\n",
       "      <td>0.579538</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.910902</td>\n",
       "      <td>12116.752056</td>\n",
       "      <td>0</td>\n",
       "      <td>6018.596934</td>\n",
       "      <td>0.583294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.916387</td>\n",
       "      <td>12492.439673</td>\n",
       "      <td>0</td>\n",
       "      <td>6206.440743</td>\n",
       "      <td>0.570356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.915715</td>\n",
       "      <td>12467.651547</td>\n",
       "      <td>0</td>\n",
       "      <td>6194.046680</td>\n",
       "      <td>0.571098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.914148</td>\n",
       "      <td>12319.215045</td>\n",
       "      <td>0</td>\n",
       "      <td>6119.828429</td>\n",
       "      <td>0.576339</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.912693</td>\n",
       "      <td>12265.941719</td>\n",
       "      <td>0</td>\n",
       "      <td>6093.191766</td>\n",
       "      <td>0.578139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.916163</td>\n",
       "      <td>12402.000510</td>\n",
       "      <td>0</td>\n",
       "      <td>6161.221161</td>\n",
       "      <td>0.573465</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.912693</td>\n",
       "      <td>12349.413806</td>\n",
       "      <td>0</td>\n",
       "      <td>6134.927809</td>\n",
       "      <td>0.575253</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.914036</td>\n",
       "      <td>12350.968950</td>\n",
       "      <td>0</td>\n",
       "      <td>6135.705381</td>\n",
       "      <td>0.575213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.918178</td>\n",
       "      <td>12484.290543</td>\n",
       "      <td>0</td>\n",
       "      <td>6202.366178</td>\n",
       "      <td>0.570586</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.913365</td>\n",
       "      <td>12352.517831</td>\n",
       "      <td>0</td>\n",
       "      <td>6136.479822</td>\n",
       "      <td>0.575078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.913029</td>\n",
       "      <td>12244.423172</td>\n",
       "      <td>0</td>\n",
       "      <td>6082.432492</td>\n",
       "      <td>0.578812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.912469</td>\n",
       "      <td>12272.650784</td>\n",
       "      <td>0</td>\n",
       "      <td>6096.546298</td>\n",
       "      <td>0.577934</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.908552</td>\n",
       "      <td>12130.090071</td>\n",
       "      <td>0</td>\n",
       "      <td>6025.265942</td>\n",
       "      <td>0.582858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.913141</td>\n",
       "      <td>12349.412913</td>\n",
       "      <td>0</td>\n",
       "      <td>6134.927363</td>\n",
       "      <td>0.575230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.916387</td>\n",
       "      <td>12464.894299</td>\n",
       "      <td>0</td>\n",
       "      <td>6192.668056</td>\n",
       "      <td>0.571262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.917618</td>\n",
       "      <td>12488.251874</td>\n",
       "      <td>0</td>\n",
       "      <td>6204.346843</td>\n",
       "      <td>0.570377</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.908664</td>\n",
       "      <td>12128.509741</td>\n",
       "      <td>0</td>\n",
       "      <td>6024.475777</td>\n",
       "      <td>0.582858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.918961</td>\n",
       "      <td>12528.837343</td>\n",
       "      <td>0</td>\n",
       "      <td>6224.639578</td>\n",
       "      <td>0.568958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70-30</td>\n",
       "      <td>0.913477</td>\n",
       "      <td>12321.831178</td>\n",
       "      <td>0</td>\n",
       "      <td>6121.136495</td>\n",
       "      <td>0.576207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Condition  Accuracy           BIC  F1  negative loglikelihood  pseudo-R2  \\\n",
       "0      70-30  0.917395  21443.296522   0            10679.365045   0.604736   \n",
       "0      70-30  0.920328  21476.735870   0            10696.084719   0.604125   \n",
       "0      70-30  0.919131  21516.728687   0            10716.081128   0.603373   \n",
       "0      70-30  0.921166  21633.695044   0            10774.564306   0.601216   \n",
       "0      70-30  0.921944  21636.795112   0            10776.114340   0.601157   \n",
       "0      70-30  0.918891  21548.051488   0            10731.742528   0.602749   \n",
       "0      70-30  0.918233  21440.499153   0            10677.966361   0.604790   \n",
       "0      70-30  0.920687  21524.305845   0            10719.869707   0.603222   \n",
       "0      70-30  0.919370  21493.868466   0            10704.651017   0.603801   \n",
       "0      70-30  0.921585  21609.055473   0            10762.244520   0.601671   \n",
       "0      70-30  0.918891  21391.003669   0            10653.218619   0.605702   \n",
       "0      70-30  0.920867  21456.874058   0            10686.153813   0.604493   \n",
       "0      70-30  0.918592  21323.309133   0            10619.371351   0.606952   \n",
       "0      70-30  0.918472  21499.168802   0            10707.301185   0.603707   \n",
       "0      70-30  0.920029  21489.962001   0            10702.697785   0.603871   \n",
       "0      70-30  0.919849  21494.238795   0            10704.836182   0.603787   \n",
       "0      70-30  0.920867  21604.122741   0            10759.778155   0.601765   \n",
       "0      70-30  0.918712  21356.853849   0            10636.143709   0.606341   \n",
       "0      70-30  0.917036  21221.120032   0            10568.276800   0.608835   \n",
       "0      70-30  0.921525  21724.438519   0            10819.936043   0.599540   \n",
       "0      70-30  0.923082  21712.622336   0            10814.027952   0.599756   \n",
       "0      70-30  0.922363  21799.483833   0            10857.458701   0.598121   \n",
       "0      70-30  0.918413  21412.498269   0            10663.965918   0.605307   \n",
       "0      70-30  0.920687  21616.859048   0            10766.146308   0.601519   \n",
       "0      70-30  0.917156  21378.689081   0            10647.061324   0.605933   \n",
       "0      70-30  0.920388  21617.627384   0            10766.530476   0.601516   \n",
       "0      70-30  0.919550  21491.020866   0            10703.227217   0.603851   \n",
       "0      70-30  0.917994  21441.684882   0            10678.559225   0.604767   \n",
       "0      70-30  0.918532  21460.164965   0            10687.799267   0.604424   \n",
       "0      70-30  0.921525  21697.710132   0            10806.571850   0.600031   \n",
       "..       ...       ...           ...  ..                     ...        ...   \n",
       "0      70-30  0.909671  12115.468357   0             6017.955085   0.583357   \n",
       "0      70-30  0.918178  12514.467371   0             6217.454592   0.569543   \n",
       "0      70-30  0.916835  12401.209511   0             6160.825662   0.573457   \n",
       "0      70-30  0.913029  12245.311720   0             6082.876766   0.578831   \n",
       "0      70-30  0.915603  12464.500849   0             6192.471331   0.571208   \n",
       "0      70-30  0.911686  12294.668497   0             6107.555155   0.577095   \n",
       "0      70-30  0.911798  12270.338363   0             6095.390088   0.577924   \n",
       "0      70-30  0.916163  12428.511824   0             6174.476818   0.572446   \n",
       "0      70-30  0.914260  12344.347092   0             6132.394452   0.575440   \n",
       "0      70-30  0.920193  12569.470209   0             6244.956011   0.567677   \n",
       "0      70-30  0.911686  12225.162477   0             6072.802145   0.579538   \n",
       "0      70-30  0.910902  12116.752056   0             6018.596934   0.583294   \n",
       "0      70-30  0.916387  12492.439673   0             6206.440743   0.570356   \n",
       "0      70-30  0.915715  12467.651547   0             6194.046680   0.571098   \n",
       "0      70-30  0.914148  12319.215045   0             6119.828429   0.576339   \n",
       "0      70-30  0.912693  12265.941719   0             6093.191766   0.578139   \n",
       "0      70-30  0.916163  12402.000510   0             6161.221161   0.573465   \n",
       "0      70-30  0.912693  12349.413806   0             6134.927809   0.575253   \n",
       "0      70-30  0.914036  12350.968950   0             6135.705381   0.575213   \n",
       "0      70-30  0.918178  12484.290543   0             6202.366178   0.570586   \n",
       "0      70-30  0.913365  12352.517831   0             6136.479822   0.575078   \n",
       "0      70-30  0.913029  12244.423172   0             6082.432492   0.578812   \n",
       "0      70-30  0.912469  12272.650784   0             6096.546298   0.577934   \n",
       "0      70-30  0.908552  12130.090071   0             6025.265942   0.582858   \n",
       "0      70-30  0.913141  12349.412913   0             6134.927363   0.575230   \n",
       "0      70-30  0.916387  12464.894299   0             6192.668056   0.571262   \n",
       "0      70-30  0.917618  12488.251874   0             6204.346843   0.570377   \n",
       "0      70-30  0.908664  12128.509741   0             6024.475777   0.582858   \n",
       "0      70-30  0.918961  12528.837343   0             6224.639578   0.568958   \n",
       "0      70-30  0.913477  12321.831178   0             6121.136495   0.576207   \n",
       "\n",
       "    stay  switch  \n",
       "0    NaN     NaN  \n",
       "0    NaN     1.0  \n",
       "0    NaN     1.0  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     1.0  \n",
       "0    NaN     1.0  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     1.0  \n",
       "0    NaN     NaN  \n",
       "0    NaN     1.0  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     1.0  \n",
       "0    NaN     NaN  \n",
       "0    NaN     1.0  \n",
       "0    NaN     1.0  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "..   ...     ...  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "0    NaN     NaN  \n",
       "\n",
       "[90 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'condition':['90-10','90-10','90-10','80-20','80-20','80-20','70-30','70-30','70-30'],\n",
    "    'accuracy': [0.91,0.98,0.18,0.90,0.99,0.10,0.90,0.99,0.05],\n",
    "     'type':['overall','stay','switch','overall','stay','switch','overall','stay','switch']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x110b9add8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAF/CAYAAAA/5HFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQJKkGvr/atrx0afJwhAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1113b0c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "master_stats = pd.DataFrame(data=d,index=None)\n",
    "plt.figure(figsize=(3,3))\n",
    "fontsize=14\n",
    "sns.set_style('whitegrid')\n",
    "sns.barplot(x='condition',y='accuracy',data=master_stats,hue='type',\n",
    "            palette=sns.light_palette('black'))\n",
    "plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.yticks([0,0.25,0.5,0.75,1],fontsize=fontsize)\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize,bbox_to_anchor=(1.955,-0.1))\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "#sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAE7CAYAAACmKfb6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDDkiEhpDjoiE9n8d42weYS4p9gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1113b0eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "sns.set(font_scale=1.3)\n",
    "sns.set_style('white')\n",
    "sns.barplot(x='Condition',y='F1',data=stats_0)\n",
    "plt.yticks([0,0.1,0.2,0.3])\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing beta coefficients across conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coefs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8ec363581d29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m plt.errorbar(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 90].iloc[:,1:].mean(),\n\u001b[0m\u001b[1;32m      6\u001b[0m              \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcoefs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoefs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Condition'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoefs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Condition'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m              color='red',alpha=0.5,linewidth=5)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'coefs' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1107d84e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style('white')\n",
    "fontsize = 15\n",
    "plt.figure(figsize=(3,3))\n",
    "\n",
    "plt.errorbar(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 90].iloc[:,1:].mean(),\n",
    "             yerr= coefs[coefs['Condition'] == 90].iloc[:,1:].mean() / np.sqrt(np.sum(coefs['Condition'] == 90)),\n",
    "             color='red',alpha=0.5,linewidth=5)\n",
    "plt.errorbar(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 80].iloc[:,1:].mean(),\n",
    "             yerr= coefs[coefs['Condition'] == 90].iloc[:,1:].mean() / np.sqrt(np.sum(coefs['Condition'] == 90)),\n",
    "             color='green',alpha=0.5,linewidth=5)\n",
    "plt.errorbar(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 70].iloc[:,1:].mean(),\n",
    "             yerr= coefs[coefs['Condition'] == 90].iloc[:,1:].mean() / np.sqrt(np.sum(coefs['Condition'] == 90)),\n",
    "             color='navy',alpha=0.5,linewidth=5)\n",
    "\n",
    "plt.scatter(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 90].iloc[:,1:].mean(),\n",
    "            color='red',label='90-10',s=100)\n",
    "plt.scatter(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 80].iloc[:,1:].mean(),\n",
    "            color='green',label='80-20',s=100)\n",
    "plt.scatter(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 70].iloc[:,1:].mean(),\n",
    "            color='navy',label='70-30',s=100)\n",
    "\n",
    "plt.legend(loc='upper left',fontsize=fontsize)\n",
    "#plt.xticks(np.arange(coefs.shape[1]),coefs.columns.values[1:],rotation='vertical')\n",
    "plt.xlabel('')\n",
    "plt.yticks([0,1,2],fontsize=fontsize)\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.ylabel('coefficient value',fontsize=fontsize)\n",
    "plt.xlim(-0.5,8.5)\n",
    "plt.title('Regression Coefficients',fontsize=fontsize)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test number of parameters / model flexibility vs BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = data_80.copy()\n",
    "stats = pd.DataFrame(columns=['Accuracy','BIC','negative loglikelihood','pseudo-R2','No. parameters','F1'])\n",
    "\n",
    "for i,n in enumerate(np.arange(10,0,-1)):\n",
    "    \n",
    "    for j in enumerate(range(30)):\n",
    "        model_curr,stats_curr,coefs_curr = logreg_and_eval(d,num_rewards=n)\n",
    "        models.append(models)\n",
    "        stats_curr['No. parameters'] = n\n",
    "        stats = stats.append(stats_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "sns.factorplot(x='No. parameters',y='BIC',data=stats,color='black')\n",
    "plt.title('BIC vs model flexibility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BIC = np.zeros((10,10))\n",
    "F1 = np.zeros((10,10))\n",
    "R2 = np.zeros((10,10))\n",
    "acc = np.zeros((10,10))\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if not ((j==0) & (i==0)):\n",
    "            model_curr,stats_curr,coefs_curr = logreg_and_eval_withports(d,num_rewards=i,num_ports=j)\n",
    "            BIC[i,j] = stats_curr['BIC'].values\n",
    "            F1[i,j] = stats_curr['F1'].values\n",
    "            R2[i,j] = stats_curr['pseudo-R2'].values\n",
    "            acc[i,j] = stats_curr['Accuracy'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(F1)\n",
    "plt.xlabel('# previous decisions',fontsize=15)\n",
    "plt.ylabel('# previous reward outcomes',fontsize=15)\n",
    "plt.title('F1',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training / testing on different conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train on 90-10, test on 80-20 and 70-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,d in enumerate([data_90,data_80,data_70]):\n",
    "    \n",
    "    if i == 0:\n",
    "        model,stats,coefs = logreg_and_eval(d)\n",
    "    else:\n",
    "        model,stats_curr,coefs_curr = logreg_and_eval(data_90,test_data = d)\n",
    "        \n",
    "        stats = stats.append(stats_curr)\n",
    "        coefs = coefs.append(coefs_curr)\n",
    "\n",
    "stats_90 = stats.drop(['BIC','negative loglikelihood','pseudo-R2'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats_90['Testing Condition'] = [90,80,70]\n",
    "stats_90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is interesting - accuracy stays pretty much the same across conditions, but F1 goes way down. And if we take a look at the confusion tables above, we can see it is because the accuracy on the switches went down (and accuracy on stays went up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train on 80-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,d in enumerate([data_80,data_90,data_70]):\n",
    "    \n",
    "    if i == 0:\n",
    "        model,stats,coefs = logreg_and_eval(d)\n",
    "    else:\n",
    "        model,stats_curr,coefs_curr = logreg_and_eval(data_80,test_data = d)\n",
    "        \n",
    "        stats = stats.append(stats_curr)\n",
    "        coefs = coefs.append(coefs_curr)\n",
    "\n",
    "stats_80 = stats.drop(['BIC','negative loglikelihood','pseudo-R2'],axis=1)\n",
    "\n",
    "stats_80['Testing Condition'] = [80,90,70]\n",
    "stats_80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train on 70-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,d in enumerate([data_70,data_80,data_90]):\n",
    "    \n",
    "    if i == 0:\n",
    "        model,stats,coefs = logreg_and_eval(d)\n",
    "    else:\n",
    "        model,stats_curr,coefs_curr = logreg_and_eval(data_70,test_data = d)\n",
    "        \n",
    "        stats = stats.append(stats_curr)\n",
    "        coefs = coefs.append(coefs_curr)\n",
    "\n",
    "stats_70 = stats.drop(['BIC','negative loglikelihood','pseudo-R2'],axis=1)\n",
    "\n",
    "stats_70['Testing Condition'] = [70,80,90]\n",
    "stats_70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1s = np.vstack((stats_90['F1'].values,\n",
    "               stats_80['F1'].values[[1,0,2]],\n",
    "               stats_70['F1'].values[[2,1,0]]))\n",
    "sns.heatmap(f1s,vmin=0,vmax=0.4)\n",
    "plt.xticks([0.5,1.5,2.5],['90','80','70'])\n",
    "plt.yticks([0.5,1.5,2.5],['70','80','90'])\n",
    "plt.ylabel('Testing Condition')\n",
    "plt.xlabel('Training Condition')\n",
    "plt.title('F1 scores when trained & tested on different conditions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if this is right, it means that the rules are different - what predicts a switch in 90-10 does not predict a switch in 80-20. But since most of the trials follow the last one, the accuracy doesn't drop very much. So it appears to be working fine, even though it is not. \n",
    "\n",
    "Can the difference be explained in the small differences in beta coefficient values? It must be ... what else is there? They seem similar enough that I'm surprised it makes such a difference. \n",
    "\n",
    "Let's go on to train and test on separate mice:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test on separate mice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's take a quick look at the mice's performances - specifically just at p(choose high P port):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_90.insert(0,'Condition',0.9)\n",
    "data_80.insert(0,'Condition',0.8)\n",
    "data_70.insert(0,'Condition',0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = data_90.append(data_80)\n",
    "all_data = all_data.append(data_70)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x='Condition',y='Higher p port',hue='Mouse ID',data = all_data,legend=False,size=5,aspect=1.7)\n",
    "plt.legend(bbox_to_anchor=(1.2,1))\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('Condition',fontsize=20)\n",
    "plt.ylabel('rate higher prob port chosen',fontsize=20)\n",
    "plt.title('Average p(choose better port) for each mouse across conditions',fontsize=20,x=0.5,y=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so based on this. Let's start with 80-20, and do a few different comparisons. \n",
    "\n",
    "1. Start by training with harry, and testing on all the others. \n",
    "2. Then try training on volde, testing on all others. \n",
    "3. Finally train on someone in the middle - like Tom or q45, and test on others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Loop through mice\n",
    "'''\n",
    "mice = np.unique(data_80['Mouse ID'].values)\n",
    "\n",
    "stats = pd.DataFrame(columns=['Accuracy','F1','Training Mouse','Testing Mouse'])\n",
    "test_mice = []\n",
    "train_mice = []\n",
    "\n",
    "for mouse_train in mice:\n",
    "\n",
    "    d_train = data_80[data_80['Mouse ID'] == mouse_train].copy()\n",
    "\n",
    "    for i,mouse_test in enumerate(mice):\n",
    "        d_test = data_80[data_80['Mouse ID'] == mouse_test].copy()\n",
    "\n",
    "        if i == 0:\n",
    "            model,stats_curr,coefs = logreg_and_eval(d_train,test_data = d_test)\n",
    "            stats_curr = stats_curr.drop(['BIC','negative loglikelihood','pseudo-R2'],axis=1)\n",
    "            stats = stats.append(stats_curr)\n",
    "        else:\n",
    "            model,stats_curr,coefs_curr = logreg_and_eval(d_train,test_data = d_test)\n",
    "            stats_curr = stats_curr.drop(['BIC','negative loglikelihood','pseudo-R2'],axis=1)\n",
    "            stats= stats.append(stats_curr)\n",
    "            coefs = coefs.append(coefs_curr)\n",
    "\n",
    "        test_mice.append(mouse_test)\n",
    "        train_mice.append(mouse_train)\n",
    "\n",
    "stats['Testing Mouse'] = test_mice\n",
    "stats['Training Mouse'] = train_mice\n",
    "acc_matrix = np.reshape(stats['Accuracy'].values,(len(mice),-1)).T\n",
    "F1_matrix = np.reshape(stats['F1'].values,(len(mice),-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fontsize=20\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "sns.set(font_scale=1.8)\n",
    "sns.heatmap(acc_matrix)\n",
    "plt.xticks(np.arange(11)+0.5,rotation='horizontal',fontsize=fontsize)\n",
    "plt.yticks(np.arange(11)+0.5,rotation='horizontal',fontsize=fontsize)\n",
    "plt.xlabel('Training Mouse',fontsize=fontsize)\n",
    "plt.ylabel('Testing Mouse',fontsize=fontsize)\n",
    "plt.title('Accuracy',fontsize=fontsize)\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.heatmap(F1_matrix)\n",
    "plt.xticks(np.arange(11)+0.5,rotation='horizontal',fontsize=fontsize)\n",
    "plt.yticks(np.arange(11)+0.5,rotation='horizontal',fontsize=fontsize)\n",
    "plt.xlabel('Training Mouse',fontsize=fontsize)\n",
    "plt.ylabel('Testing Mouse',fontsize=20)\n",
    "plt.title('F1',fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_80.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u_switch = np.zeros(len(mice))\n",
    "u_acc = np.zeros(len(mice))\n",
    "\n",
    "for i,mouse in enumerate(mice):\n",
    "    u_switch[i] = data_80[data_80['Mouse ID']== mouse]['Switch'].mean()\n",
    "    u_acc[i] = data_80[data_80['Mouse ID']== mouse]['Higher p port'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette('hls',n_colors=len(mice))\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.suptitle('Higher switch rates anti-correlate with accuracy on test mice',fontsize=20,x=0.5,y=1.1)\n",
    "plt.subplot(121)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_switch[i],acc_matrix[i,:].mean(),label=mouse,c=colors[i],s=50)\n",
    "#plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean switch rate')\n",
    "plt.ylabel('mean accuracy when used as test mouse')\n",
    "plt.ylim(0.75,1)\n",
    "plt.xlim(0,0.22)\n",
    "plt.title('Testing accuracy vs mean switch rate')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_switch[i],acc_matrix[:,i].mean(),label=mouse,c=colors[i],s=50)\n",
    "plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean switch rate')\n",
    "plt.ylabel('mean accuracy when used as train mouse')\n",
    "plt.ylim(0.75,1)\n",
    "plt.xlim(0,0.22)\n",
    "plt.title('Training accuracy vs mean switch rate')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette('hls',n_colors=len(mice))\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.suptitle('Higher switch rates correlate with F1 score on test mice',fontsize=20,x=0.5,y=1.1)\n",
    "plt.subplot(121)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_switch[i],F1_matrix[i,:].mean(),label=mouse,c=colors[i],s=50)\n",
    "#plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean switch rate')\n",
    "plt.ylabel('mean F1 when used as test mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Testing F1 vs mean switch rate')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_switch[i],F1_matrix[:,i].mean(),label=mouse,c=colors[i],s=50)\n",
    "plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean switch rate')\n",
    "plt.ylabel('mean F1 when used as train mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Training F1 vs mean switch rate')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette('hls',n_colors=len(mice))\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.suptitle('Better behavior correlates with accuracy score on test mice',fontsize=20,x=0.5,y=1.1)\n",
    "plt.subplot(121)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_acc[i],acc_matrix[i,:].mean(),label=mouse,c=colors[i],s=50)\n",
    "#plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean p(high p port)')\n",
    "plt.ylabel('mean acc when used as test mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Testing Acc vs mean p(high p port)')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_acc[i],acc_matrix[:,i].mean(),label=mouse,c=colors[i],s=50)\n",
    "plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean p(high p port)')\n",
    "plt.ylabel('mean acc when used as train mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Training acc vs mean p(high p port)')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette('hls',n_colors=len(mice))\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.suptitle('p(high p port) vs F1 score',fontsize=20,x=0.5,y=1.1)\n",
    "plt.subplot(121)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_acc[i],F1_matrix[i,:].mean(),label=mouse,c=colors[i],s=50)\n",
    "#plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('p(high p port)')\n",
    "plt.ylabel('mean F1 when used as test mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Testing F1 vs mean switch rate')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_acc[i],F1_matrix[:,i].mean(),label=mouse,c=colors[i],s=50)\n",
    "plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('p(high p port)')\n",
    "plt.ylabel('mean F1 when used as train mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Training F1 vs mean switch rate')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2,interaction_only=True,include_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logreg_and_eval_withports_and_interactions(data,num_rewards=10,num_ports=1,test_data=False):\n",
    "    '''\n",
    "    Perform Logistic Regression on a pandas dataframe of trials (from feature matrix) with interactions\n",
    "    \n",
    "    Inputs:\n",
    "        - data: pandas dataframe of trials (from feature matrix)\n",
    "    Outputs:\n",
    "        - logreg: trained logistic regression model (from sklearn)\n",
    "        - stats:  pandas dataframe with F1, pseudo-R2, and BIC scores from model\n",
    "        - coeffs: beta coefficients from logreg\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    from statsmodels.discrete.discrete_model import Logit\n",
    "    \n",
    "    port_features = []\n",
    "    reward_features = []\n",
    "\n",
    "    #change right port to -1 instead of 0\n",
    "    for col in data:\n",
    "        if '_Port' in col:\n",
    "            data.loc[data[col] == 0,col] = -1\n",
    "            port_features.append(col)\n",
    "        elif '_Reward' in col:\n",
    "            reward_features.append(col)\n",
    "\n",
    "    #create new feature matrix\n",
    "    d = data.copy()\n",
    "    for i in range(len(port_features)):\n",
    "        d[reward_features[i]] = d[reward_features[i]].values*d[port_features[i]].values\n",
    "    \n",
    "    \n",
    "    #determine the features\n",
    "    features = reward_features.copy()\n",
    "    if num_rewards == 0:\n",
    "        features = port_features[-1*num_ports:]\n",
    "    elif num_ports == 0:\n",
    "        features = features[-1*num_rewards:] #only take the num of rewards specificied in the function\n",
    "    else:\n",
    "        features = features[-1*num_rewards:]\n",
    "        features = np.append(features,port_features[-1*num_ports:])\n",
    "    \n",
    "    print(features)\n",
    "    features = np.append(features,'Decision') #finally append the decision so we can take it to predict later\n",
    "    \n",
    "    \n",
    "    \n",
    "    #final version of data\n",
    "    d = d[features].copy() #this now just has the features we want and the decision we want to predict\n",
    "    \n",
    "    #do the same thing for the test data if it exists!\n",
    "    if test_data is not False:\n",
    "        for col in test_data:\n",
    "            if '_Port' in col:\n",
    "                test_data.loc[test_data[col] == 0,col] = -1\n",
    "\n",
    "        #create new feature matrix\n",
    "        data_test_new = test_data.copy()\n",
    "        for i in range(len(port_features)):\n",
    "            data_test_new[reward_features[i]] = test_data[reward_features[i]].values*test_data[port_features[i]].values\n",
    "        \n",
    "        d_test = data_test_new[features].copy()\n",
    "    \n",
    "    \n",
    "        #set training and testing sets now\n",
    "        x_train = d.iloc[:,:-1].values\n",
    "        y_train = d.iloc[:,-1].values\n",
    "        x_test = d_test.iloc[:,:-1].values\n",
    "        y_test = d_test.iloc[:,-1].values\n",
    "        \n",
    "        prev_port_test = test_data['1_Port'].values\n",
    "        prev_port_test[prev_port_test==-1] = 0\n",
    "    \n",
    "    #if there is no test data, then split up the data into training and testing\n",
    "    else:\n",
    "        #extract features and decisions\n",
    "        x = d.iloc[:,:-1].values\n",
    "        y = d.iloc[:,-1].values\n",
    "\n",
    "        #split into training and testing\n",
    "        n_trials = x.shape[0]\n",
    "        shuf_inds = np.random.permutation(n_trials)\n",
    "        split_ind = int(n_trials*0.7)\n",
    "\n",
    "        x_train = x[shuf_inds[:split_ind],:]\n",
    "        y_train = y[shuf_inds[:split_ind]]\n",
    "\n",
    "        x_test = x[shuf_inds[split_ind:],:]\n",
    "        y_test = y[shuf_inds[split_ind:]]\n",
    "        \n",
    "        #extract previous port decision for test set\n",
    "        #these will be used to calculate switches on the test predictions\n",
    "        prev_port_test = data['1_Port'].values[shuf_inds[split_ind:]]\n",
    "        prev_port_test[prev_port_test==-1] = 0\n",
    "    \n",
    "    '''\n",
    "    Modeling\n",
    "    '''\n",
    "    \n",
    "    #create interaction terms\n",
    "    poly = PolynomialFeatures(degree=5,interaction_only=True,include_bias=True)\n",
    "    x_train = poly.fit_transform(x_train)\n",
    "    x_test = poly.fit_transform(x_test)\n",
    "    print('x_train shape: %.0f' % x_train.shape[1])\n",
    "    \n",
    "    \n",
    "    #fit logistic regression\n",
    "    logreg = sklearn.linear_model.LogisticRegressionCV()\n",
    "    logreg.fit(x_train,y_train)\n",
    "    \n",
    "    #predict on testing set\n",
    "    y_predict = logreg.predict(x_test)\n",
    "    y_predict_proba = logreg.predict_proba(x_test)\n",
    "    \n",
    "    #model accuracy\n",
    "    score = logreg.score(x_test,y_test)\n",
    "    \n",
    "    #calculating pseudo-R2 and BIC from statsmodel OLS\n",
    "    model = Logit(y_train,x_train)\n",
    "    rslt  = model.fit()\n",
    "\n",
    "    #switches\n",
    "    y_test_switch = np.abs(y_test - prev_port_test)\n",
    "    y_predict_switch = np.abs(y_predict - prev_port_test)\n",
    "    acc_pos,acc_neg,F1=sf.score_both_and_confuse(y_predict_switch,y_test_switch,confusion=False,disp=True)\n",
    "    \n",
    "    #extract coefficients\n",
    "    coefs = logreg.coef_ #retrieve coefs\n",
    "    coefs = np.append(coefs[0],logreg.intercept_) #add bias coef\n",
    "    \n",
    "    #create stats database to return\n",
    "    d_ = {'pseudo-R2':rslt.prsquared,'stay':acc_pos,'switch':acc_neg,'Accuracy':score,'BIC':rslt.bic,'negative loglikelihood':-1*rslt.llf}\n",
    "    stats = pd.DataFrame(data=d_,index=[0])\n",
    "    features = features[:-1]\n",
    "    features = np.append(features,'Bias')\n",
    "    \n",
    "    #coefs = pd.DataFrame(data=coefs.reshape(1,-1),columns=poly.get_feature_names())\n",
    "    return logreg,stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model,stats_curr = logreg_and_eval_withports_and_interactions(data_80,num_rewards = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.coef_.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(model.coef_.shape[1]),model.coef_)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:CaGRIN]",
   "language": "python",
   "name": "conda-env-CaGRIN-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
