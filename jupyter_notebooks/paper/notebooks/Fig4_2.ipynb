{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "This logistic regression is based on the Beeler/Daw et al. 2010 paper.\n",
    "Specifically:\n",
    "\n",
    "dependent variable: binary choice of port (-1 or 1)\n",
    "\n",
    "explanatory variables: \n",
    "1. the N previous rewards $ r_{t-N:t-1} $\n",
    "2. the previous choice $c_{t-1}$ to capture a tendency to stay or switch\n",
    "3. bias variable (1) to capture fixed, overall preference for either port\n",
    "\n",
    "Note: this model only carries information about ports when it gets a reward. IE -1 = right reward, 1 = left reward, but 0 = no reward (for either side). Should compare models with this information vs. added information about the non-rewarded port choices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/shayneufeld/GitHub/mouse_bandit/data_preprocessing_code')\n",
    "sys.path.append('/Users/shayneufeld/GitHub/mouse_bandit')\n",
    "import support_functions as sf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import bandit_preprocessing as bp\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define function to do logistic regression and some basic evalutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "I compiled the code above into a more compact function so I can cycle through different conditions/mice/etc \n",
    "as neccessary\n",
    "'''\n",
    "\n",
    "def logreg_and_eval(data,num_rewards=10,test_data=False):\n",
    "    '''\n",
    "    Perform Logistic Regression on a pandas dataframe of trials (from feature matrix)\n",
    "    \n",
    "    Inputs:\n",
    "        - data: pandas dataframe of trials (from feature matrix)\n",
    "    Outputs:\n",
    "        - logreg: trained logistic regression model (from sklearn)\n",
    "        - stats:  pandas dataframe with F1, pseudo-R2, and BIC scores from model\n",
    "        - coeffs: beta coefficients from logreg\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    from statsmodels.discrete.discrete_model import Logit\n",
    "    \n",
    "    port_features = []\n",
    "    reward_features = []\n",
    "\n",
    "    #change right port to -1 instead of 0\n",
    "    for col in data:\n",
    "        if '_Port' in col:\n",
    "            data.loc[data[col] == 0,col] = -1\n",
    "            port_features.append(col)\n",
    "        elif '_Reward' in col:\n",
    "            reward_features.append(col)\n",
    "\n",
    "    #create new feature matrix\n",
    "    d = data.copy()\n",
    "    for i in range(len(port_features)):\n",
    "        d[reward_features[i]] = d[reward_features[i]].values*d[port_features[i]].values\n",
    "    \n",
    "    \n",
    "    #determine the features\n",
    "    features = reward_features.copy()\n",
    "    features = features[-1*num_rewards:] #only take the num of rewards specificied in the function\n",
    "    features.append('1_Port') #append the last decision as a feature\n",
    "    features.append('Decision') #finally append the decision so we can take it to predict later\n",
    "    \n",
    "    #final version of data\n",
    "    d = d[features].copy() #this now just has the features we want and the decision we want to predict\n",
    "    \n",
    "    #do the same thing for the test data if it exists!\n",
    "    if test_data is not False:\n",
    "        for col in test_data:\n",
    "            if '_Port' in col:\n",
    "                test_data.loc[test_data[col] == 0,col] = -1\n",
    "\n",
    "        #create new feature matrix\n",
    "        data_test_new = test_data.copy()\n",
    "        for i in range(len(port_features)):\n",
    "            data_test_new[reward_features[i]] = test_data[reward_features[i]].values*test_data[port_features[i]].values\n",
    "        \n",
    "        d_test = data_test_new[features].copy()\n",
    "    \n",
    "    \n",
    "        #set training and testing sets now\n",
    "        x_train = d.iloc[:,:-1].values\n",
    "        y_train = d.iloc[:,-1].values\n",
    "        x_test = d_test.iloc[:,:-1].values\n",
    "        y_test = d_test.iloc[:,-1].values\n",
    "        \n",
    "        prev_port_test = d_test['1_Port'].values\n",
    "        prev_port_test[prev_port_test==-1] = 0\n",
    "    \n",
    "    #if there is no test data, then split up the data into training and testing\n",
    "    else:\n",
    "        #extract features and decisions\n",
    "        x = d.iloc[:,:-1].values\n",
    "        y = d.iloc[:,-1].values\n",
    "\n",
    "        #split into training and testing\n",
    "        n_trials = x.shape[0]\n",
    "        shuf_inds = np.random.permutation(n_trials)\n",
    "        split_ind = int(n_trials*0.7)\n",
    "\n",
    "        x_train = x[shuf_inds[:split_ind],:]\n",
    "        y_train = y[shuf_inds[:split_ind]]\n",
    "\n",
    "        x_test = x[shuf_inds[split_ind:],:]\n",
    "        y_test = y[shuf_inds[split_ind:]]\n",
    "        \n",
    "        #extract previous port decision for test set\n",
    "        #these will be used to calculate switches on the test predictions\n",
    "        prev_port_test = d['1_Port'].values[shuf_inds[split_ind:]]\n",
    "        prev_port_test[prev_port_test==-1] = 0\n",
    "    \n",
    "    '''\n",
    "    Modeling\n",
    "    '''\n",
    "    \n",
    "    #fit logistic regression\n",
    "    logreg = sklearn.linear_model.LogisticRegressionCV()\n",
    "    logreg.fit(x_train,y_train)\n",
    "    \n",
    "    #predict on testing set\n",
    "    y_predict = logreg.predict(x_test)\n",
    "    y_predict_proba = logreg.predict_proba(x_test)\n",
    "    \n",
    "    #model accuracy\n",
    "    score = logreg.score(x_test,y_test)\n",
    "    \n",
    "    #calculating pseudo-R2 and BIC from statsmodel OLS\n",
    "    model = Logit(y_train,x_train)\n",
    "    rslt  = model.fit()\n",
    "\n",
    "    #switches\n",
    "    y_test_switch = np.abs(y_test - prev_port_test)\n",
    "    y_predict_switch = np.abs(y_predict - prev_port_test)\n",
    "    acc_pos,acc_neg,F1=sf.score_both_and_confuse(y_predict_switch,y_test_switch,confusion=False,disp=True)\n",
    "    \n",
    "    #extract coefficients\n",
    "    coefs = logreg.coef_ #retrieve coefs\n",
    "    coefs = np.append(coefs[0],logreg.intercept_) #add bias coef\n",
    "    \n",
    "    #create stats database to return\n",
    "    d_ = {'pseudo-R2':rslt.prsquared,'stay':acc_pos,'switch':acc_neg,'Accuracy':score,\n",
    "          'BIC':rslt.bic,'negative loglikelihood':-1*rslt.llf,'F1':F1}\n",
    "    stats = pd.DataFrame(data=d_,index=[0])\n",
    "    features = features[:-1]\n",
    "    features.append('Bias')\n",
    "    \n",
    "    coefs = pd.DataFrame(data=coefs.reshape(1,-1),columns=features)\n",
    "    return logreg,stats,coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logreg_and_eval_withports(data,num_rewards=10,num_ports=1,test_data=False):\n",
    "    '''\n",
    "    Perform Logistic Regression on a pandas dataframe of trials (from feature matrix)\n",
    "    \n",
    "    Inputs:\n",
    "        - data: pandas dataframe of trials (from feature matrix)\n",
    "    Outputs:\n",
    "        - logreg: trained logistic regression model (from sklearn)\n",
    "        - stats:  pandas dataframe with F1, pseudo-R2, and BIC scores from model\n",
    "        - coeffs: beta coefficients from logreg\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    from statsmodels.discrete.discrete_model import Logit\n",
    "    \n",
    "    port_features = []\n",
    "    reward_features = []\n",
    "\n",
    "    #change right port to -1 instead of 0\n",
    "    for col in data:\n",
    "        if '_Port' in col:\n",
    "            data.loc[data[col] == 0,col] = -1\n",
    "            port_features.append(col)\n",
    "        elif '_Reward' in col:\n",
    "            reward_features.append(col)\n",
    "\n",
    "    #create new feature matrix\n",
    "    d = data.copy()\n",
    "    for i in range(len(port_features)):\n",
    "        d[reward_features[i]] = d[reward_features[i]].values*d[port_features[i]].values\n",
    "    \n",
    "    \n",
    "    #determine the features\n",
    "    features = reward_features.copy()\n",
    "    if num_rewards == 0:\n",
    "        features = port_features[-1*num_ports:]\n",
    "    elif num_ports == 0:\n",
    "        features = features[-1*num_rewards:] #only take the num of rewards specificied in the function\n",
    "    else:\n",
    "        features = features[-1*num_rewards:]\n",
    "        features = np.append(features,port_features[-1*num_ports:])\n",
    "    \n",
    "    print(features)\n",
    "    features = np.append(features,'Decision') #finally append the decision so we can take it to predict later\n",
    "    \n",
    "    \n",
    "    \n",
    "    #final version of data\n",
    "    d = d[features].copy() #this now just has the features we want and the decision we want to predict\n",
    "    \n",
    "    #do the same thing for the test data if it exists!\n",
    "    if test_data is not False:\n",
    "        for col in test_data:\n",
    "            if '_Port' in col:\n",
    "                test_data.loc[test_data[col] == 0,col] = -1\n",
    "\n",
    "        #create new feature matrix\n",
    "        data_test_new = test_data.copy()\n",
    "        for i in range(len(port_features)):\n",
    "            data_test_new[reward_features[i]] = test_data[reward_features[i]].values*test_data[port_features[i]].values\n",
    "        \n",
    "        d_test = data_test_new[features].copy()\n",
    "    \n",
    "    \n",
    "        #set training and testing sets now\n",
    "        x_train = d.iloc[:,:-1].values\n",
    "        y_train = d.iloc[:,-1].values\n",
    "        x_test = d_test.iloc[:,:-1].values\n",
    "        y_test = d_test.iloc[:,-1].values\n",
    "        \n",
    "        prev_port_test = test_data['1_Port'].values\n",
    "        prev_port_test[prev_port_test==-1] = 0\n",
    "    \n",
    "    #if there is no test data, then split up the data into training and testing\n",
    "    else:\n",
    "        #extract features and decisions\n",
    "        x = d.iloc[:,:-1].values\n",
    "        y = d.iloc[:,-1].values\n",
    "\n",
    "        #split into training and testing\n",
    "        n_trials = x.shape[0]\n",
    "        shuf_inds = np.random.permutation(n_trials)\n",
    "        split_ind = int(n_trials*0.7)\n",
    "\n",
    "        x_train = x[shuf_inds[:split_ind],:]\n",
    "        y_train = y[shuf_inds[:split_ind]]\n",
    "\n",
    "        x_test = x[shuf_inds[split_ind:],:]\n",
    "        y_test = y[shuf_inds[split_ind:]]\n",
    "        \n",
    "        #extract previous port decision for test set\n",
    "        #these will be used to calculate switches on the test predictions\n",
    "        prev_port_test = data['1_Port'].values[shuf_inds[split_ind:]]\n",
    "        prev_port_test[prev_port_test==-1] = 0\n",
    "    \n",
    "    '''\n",
    "    Modeling\n",
    "    '''\n",
    "    \n",
    "    #fit logistic regression\n",
    "    logreg = sklearn.linear_model.LogisticRegressionCV()\n",
    "    logreg.fit(x_train,y_train)\n",
    "    \n",
    "    #predict on testing set\n",
    "    y_predict = logreg.predict(x_test)\n",
    "    y_predict_proba = logreg.predict_proba(x_test)\n",
    "    \n",
    "    #model accuracy\n",
    "    score = logreg.score(x_test,y_test)\n",
    "    \n",
    "    #calculating pseudo-R2 and BIC from statsmodel OLS\n",
    "    model = Logit(y_train,x_train)\n",
    "    rslt  = model.fit()\n",
    "\n",
    "    #switches\n",
    "    y_test_switch = np.abs(y_test - prev_port_test)\n",
    "    y_predict_switch = np.abs(y_predict - prev_port_test)\n",
    "    acc_pos,acc_neg,F1=sf.score_both_and_confuse(y_predict_switch,y_test_switch,confusion=False,disp=True)\n",
    "    \n",
    "    #extract coefficients\n",
    "    coefs = logreg.coef_ #retrieve coefs\n",
    "    coefs = np.append(coefs[0],logreg.intercept_) #add bias coef\n",
    "    \n",
    "    #create stats database to return\n",
    "    d_ = {'pseudo-R2':rslt.prsquared,'stay':acc_pos,'switch':acc_neg,'Accuracy':score,\n",
    "          'BIC':rslt.bic,'negative loglikelihood':-1*rslt.llf,'F1':F1}\n",
    "    stats = pd.DataFrame(data=d_,index=[0])\n",
    "    features = features[:-1]\n",
    "    features = np.append(features,'Bias')\n",
    "    \n",
    "    coefs = pd.DataFrame(data=coefs.reshape(1,-1),columns=features)\n",
    "    return logreg,stats,coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_90 = pd.read_csv('/Users/shayneufeld/GitHub/mouse_bandit/data/processed_data/full_9010_02192017.csv',index_col=0)\n",
    "data_80 = pd.read_csv('/Users/shayneufeld/GitHub/mouse_bandit/data/processed_data/full_8020_02192017.csv',index_col=0)\n",
    "data_70 = pd.read_csv('/Users/shayneufeld/GitHub/mouse_bandit/data/processed_data/full_7030_02192017.csv',index_col=0)\n",
    "\n",
    "datas = [data_70,data_80,data_90]\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mouse ID</th>\n",
       "      <th>Session ID</th>\n",
       "      <th>Block Trial</th>\n",
       "      <th>Port Streak</th>\n",
       "      <th>Reward Streak</th>\n",
       "      <th>10_Port</th>\n",
       "      <th>10_Reward</th>\n",
       "      <th>10_ITI</th>\n",
       "      <th>10_trialDuration</th>\n",
       "      <th>9_Port</th>\n",
       "      <th>...</th>\n",
       "      <th>2_trialDuration</th>\n",
       "      <th>1_Port</th>\n",
       "      <th>1_Reward</th>\n",
       "      <th>1_ITI</th>\n",
       "      <th>1_trialDuration</th>\n",
       "      <th>0_ITI</th>\n",
       "      <th>Decision</th>\n",
       "      <th>Switch</th>\n",
       "      <th>Higher p port</th>\n",
       "      <th>Reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K1</td>\n",
       "      <td>11212016_K1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.993</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.303</td>\n",
       "      <td>0.573</td>\n",
       "      <td>1.952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K1</td>\n",
       "      <td>11212016_K1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.729</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.952</td>\n",
       "      <td>0.445</td>\n",
       "      <td>2.681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K1</td>\n",
       "      <td>11212016_K1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.073</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.681</td>\n",
       "      <td>0.594</td>\n",
       "      <td>2.017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K1</td>\n",
       "      <td>11212016_K1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.170</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.017</td>\n",
       "      <td>0.465</td>\n",
       "      <td>2.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K1</td>\n",
       "      <td>11212016_K1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.299</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.135</td>\n",
       "      <td>0.518</td>\n",
       "      <td>2.382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Mouse ID   Session ID  Block Trial  Port Streak  Reward Streak  10_Port  \\\n",
       "0       K1  11212016_K1         11.0          1.0            1.0      0.0   \n",
       "1       K1  11212016_K1         12.0          2.0            2.0      0.0   \n",
       "2       K1  11212016_K1         13.0          3.0            3.0      0.0   \n",
       "3       K1  11212016_K1         14.0          4.0            4.0      0.0   \n",
       "4       K1  11212016_K1         15.0          5.0            5.0      0.0   \n",
       "\n",
       "   10_Reward  10_ITI  10_trialDuration  9_Port   ...    2_trialDuration  \\\n",
       "0        1.0   1.993             0.727     0.0   ...              0.524   \n",
       "1        1.0   1.729             0.894     0.0   ...              0.573   \n",
       "2        1.0   2.073             0.809     0.0   ...              0.445   \n",
       "3        1.0   2.170             0.518     0.0   ...              0.594   \n",
       "4        0.0   2.299             0.524     0.0   ...              0.465   \n",
       "\n",
       "   1_Port  1_Reward  1_ITI  1_trialDuration  0_ITI  Decision  Switch  \\\n",
       "0     0.0       1.0  1.303            0.573  1.952       0.0     0.0   \n",
       "1     0.0       1.0  1.952            0.445  2.681       0.0     0.0   \n",
       "2     0.0       1.0  2.681            0.594  2.017       0.0     0.0   \n",
       "3     0.0       1.0  2.017            0.465  2.135       0.0     0.0   \n",
       "4     0.0       1.0  2.135            0.518  2.382       0.0     0.0   \n",
       "\n",
       "   Higher p port  Reward  \n",
       "0            1.0     1.0  \n",
       "1            1.0     1.0  \n",
       "2            1.0     1.0  \n",
       "3            1.0     1.0  \n",
       "4            1.0     1.0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_80.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.239732\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4848.0           28.0\n",
      "True YES         462.0           19.0\n",
      "\n",
      "F1: 0.072\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.04\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.243138\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4860.0           31.0\n",
      "True YES         433.0           33.0\n",
      "\n",
      "F1: 0.125\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.07\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.236794\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4812.0           31.0\n",
      "True YES         484.0           30.0\n",
      "\n",
      "F1: 0.104\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.06\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.239657\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4855.0           28.0\n",
      "True YES         445.0           29.0\n",
      "\n",
      "F1: 0.109\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.06\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.237314\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4831.0           17.0\n",
      "True YES         489.0           20.0\n",
      "\n",
      "F1: 0.073\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.04\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.239250\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4836.0           17.0\n",
      "True YES         474.0           30.0\n",
      "\n",
      "F1: 0.109\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.06\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.241555\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4851.0           26.0\n",
      "True YES         447.0           33.0\n",
      "\n",
      "F1: 0.122\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.07\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.237051\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4837.0           40.0\n",
      "True YES         453.0           27.0\n",
      "\n",
      "F1: 0.099\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.06\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.242792\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4860.0           38.0\n",
      "True YES         420.0           39.0\n",
      "\n",
      "F1: 0.146\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.08\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.238399\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4829.0           32.0\n",
      "True YES         470.0           26.0\n",
      "\n",
      "F1: 0.094\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.05\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.236439\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4826.0           24.0\n",
      "True YES         478.0           29.0\n",
      "\n",
      "F1: 0.104\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.06\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.239119\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4824.0           34.0\n",
      "True YES         467.0           32.0\n",
      "\n",
      "F1: 0.113\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.06\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.239008\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4834.0           42.0\n",
      "True YES         452.0           29.0\n",
      "\n",
      "F1: 0.105\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.06\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.238799\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4826.0           30.0\n",
      "True YES         477.0           24.0\n",
      "\n",
      "F1: 0.086\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.05\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.236564\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4821.0           30.0\n",
      "True YES         473.0           33.0\n",
      "\n",
      "F1: 0.116\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.07\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.240482\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4839.0           39.0\n",
      "True YES         441.0           38.0\n",
      "\n",
      "F1: 0.137\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.08\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.239334\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4827.0           46.0\n",
      "True YES         447.0           37.0\n",
      "\n",
      "F1: 0.131\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.08\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.238904\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4816.0           44.0\n",
      "True YES         461.0           36.0\n",
      "\n",
      "F1: 0.125\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.07\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.237945\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4829.0           29.0\n",
      "True YES         473.0           26.0\n",
      "\n",
      "F1: 0.094\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.05\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.237368\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4811.0           35.0\n",
      "True YES         481.0           30.0\n",
      "\n",
      "F1: 0.104\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.06\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.235826\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4831.0           17.0\n",
      "True YES         489.0           20.0\n",
      "\n",
      "F1: 0.073\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.04\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.238663\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4833.0           30.0\n",
      "True YES         465.0           29.0\n",
      "\n",
      "F1: 0.105\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.06\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.240111\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4846.0           26.0\n",
      "True YES         455.0           30.0\n",
      "\n",
      "F1: 0.111\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.06\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.243804\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4861.0           44.0\n",
      "True YES         416.0           36.0\n",
      "\n",
      "F1: 0.135\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.08\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.240080\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4845.0           32.0\n",
      "True YES         455.0           25.0\n",
      "\n",
      "F1: 0.093\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.05\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.241836\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4864.0           31.0\n",
      "True YES         448.0           14.0\n",
      "\n",
      "F1: 0.055\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.03\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.236795\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4835.0           24.0\n",
      "True YES         473.0           25.0\n",
      "\n",
      "F1: 0.091\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.05\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.239728\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4822.0           27.0\n",
      "True YES         482.0           26.0\n",
      "\n",
      "F1: 0.093\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.05\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.241510\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4864.0           30.0\n",
      "True YES         440.0           23.0\n",
      "\n",
      "F1: 0.089\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.05\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.238296\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         4836.0           31.0\n",
      "True YES         457.0           33.0\n",
      "\n",
      "F1: 0.119\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.07\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.247589\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13074.0          124.0\n",
      "True YES        1322.0          134.0\n",
      "\n",
      "F1: 0.156\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.09\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.248503\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13098.0          123.0\n",
      "True YES        1298.0          135.0\n",
      "\n",
      "F1: 0.160\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.09\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.247209\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13056.0          117.0\n",
      "True YES        1337.0          144.0\n",
      "\n",
      "F1: 0.165\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.10\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.247730\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13099.0          122.0\n",
      "True YES        1313.0          120.0\n",
      "\n",
      "F1: 0.143\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.08\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.243899\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        12990.0          185.0\n",
      "True YES        1310.0          169.0\n",
      "\n",
      "F1: 0.184\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.11\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.249201\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13079.0          143.0\n",
      "True YES        1264.0          168.0\n",
      "\n",
      "F1: 0.193\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.12\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.244829\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13005.0          133.0\n",
      "True YES        1374.0          142.0\n",
      "\n",
      "F1: 0.159\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.09\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.246292\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13033.0          137.0\n",
      "True YES        1328.0          156.0\n",
      "\n",
      "F1: 0.176\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.11\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.250458\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13071.0          154.0\n",
      "True YES        1262.0          167.0\n",
      "\n",
      "F1: 0.191\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.12\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.245693\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13047.0          119.0\n",
      "True YES        1347.0          141.0\n",
      "\n",
      "F1: 0.161\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.09\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.245580\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        12999.0          149.0\n",
      "True YES        1339.0          167.0\n",
      "\n",
      "F1: 0.183\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.11\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.250715\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13082.0          169.0\n",
      "True YES        1245.0          158.0\n",
      "\n",
      "F1: 0.183\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.11\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.248900\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13061.0          160.0\n",
      "True YES        1278.0          155.0\n",
      "\n",
      "F1: 0.177\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.11\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.247612\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13036.0          129.0\n",
      "True YES        1340.0          149.0\n",
      "\n",
      "F1: 0.169\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.10\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.250210\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13097.0          157.0\n",
      "True YES        1245.0          155.0\n",
      "\n",
      "F1: 0.181\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.11\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.248529\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13075.0          111.0\n",
      "True YES        1328.0          140.0\n",
      "\n",
      "F1: 0.163\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.10\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.244256\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13010.0          149.0\n",
      "True YES        1329.0          166.0\n",
      "\n",
      "F1: 0.183\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.11\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.249021\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13081.0          122.0\n",
      "True YES        1292.0          159.0\n",
      "\n",
      "F1: 0.184\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.11\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.249513\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13112.0          101.0\n",
      "True YES        1312.0          129.0\n",
      "\n",
      "F1: 0.154\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.09\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.249433\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13112.0          117.0\n",
      "True YES        1279.0          146.0\n",
      "\n",
      "F1: 0.173\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.10\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.246665\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13077.0          114.0\n",
      "True YES        1335.0          128.0\n",
      "\n",
      "F1: 0.150\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.09\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.245834\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13004.0          156.0\n",
      "True YES        1334.0          160.0\n",
      "\n",
      "F1: 0.177\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.11\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.247602\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13032.0          147.0\n",
      "True YES        1306.0          169.0\n",
      "\n",
      "F1: 0.189\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.11\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.246191\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13045.0          105.0\n",
      "True YES        1369.0          135.0\n",
      "\n",
      "F1: 0.155\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.09\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.248359\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13049.0          161.0\n",
      "True YES        1294.0          150.0\n",
      "\n",
      "F1: 0.171\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.10\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.247889\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13054.0          148.0\n",
      "True YES        1294.0          158.0\n",
      "\n",
      "F1: 0.180\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.11\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.252621\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13123.0          154.0\n",
      "True YES        1202.0          175.0\n",
      "\n",
      "F1: 0.205\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.13\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.249877\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13103.0          108.0\n",
      "True YES        1304.0          139.0\n",
      "\n",
      "F1: 0.164\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.10\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.245303\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13064.0          138.0\n",
      "True YES        1314.0          138.0\n",
      "\n",
      "F1: 0.160\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.10\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.248271\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO        13076.0          125.0\n",
      "True YES        1315.0          138.0\n",
      "\n",
      "F1: 0.161\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.09\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.220878\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2931.0           57.0\n",
      "True YES         195.0           53.0\n",
      "\n",
      "F1: 0.296\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.21\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.210483\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2892.0           54.0\n",
      "True YES         237.0           53.0\n",
      "\n",
      "F1: 0.267\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.18\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.217323\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2928.0           24.0\n",
      "True YES         235.0           49.0\n",
      "\n",
      "F1: 0.275\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.17\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.216848\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2923.0           43.0\n",
      "True YES         214.0           56.0\n",
      "\n",
      "F1: 0.304\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.21\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.217052\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2929.0           43.0\n",
      "True YES         219.0           45.0\n",
      "\n",
      "F1: 0.256\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.17\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.217720\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2906.0           57.0\n",
      "True YES         215.0           58.0\n",
      "\n",
      "F1: 0.299\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.21\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.211591\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2902.0           38.0\n",
      "True YES         242.0           54.0\n",
      "\n",
      "F1: 0.278\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.18\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.216797\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2911.0           54.0\n",
      "True YES         209.0           62.0\n",
      "\n",
      "F1: 0.320\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.23\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.216457\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2906.0           50.0\n",
      "True YES         224.0           56.0\n",
      "\n",
      "F1: 0.290\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.20\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.223713\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2947.0           46.0\n",
      "True YES         191.0           52.0\n",
      "\n",
      "F1: 0.305\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.21\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.219195\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2931.0           53.0\n",
      "True YES         204.0           48.0\n",
      "\n",
      "F1: 0.272\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.19\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.220303\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2934.0           59.0\n",
      "True YES         187.0           56.0\n",
      "\n",
      "F1: 0.313\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.23\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.209078\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2893.0           48.0\n",
      "True YES         240.0           55.0\n",
      "\n",
      "F1: 0.276\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.19\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.213337\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2918.0           39.0\n",
      "True YES         244.0           35.0\n",
      "\n",
      "F1: 0.198\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.13\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.217389\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2914.0           66.0\n",
      "True YES         208.0           48.0\n",
      "\n",
      "F1: 0.259\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.19\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.217365\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2914.0           56.0\n",
      "True YES         208.0           58.0\n",
      "\n",
      "F1: 0.305\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.22\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.212409\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2909.0           30.0\n",
      "True YES         255.0           42.0\n",
      "\n",
      "F1: 0.228\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.14\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.209365\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2890.0           54.0\n",
      "True YES         243.0           49.0\n",
      "\n",
      "F1: 0.248\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.17\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.212460\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2911.0           46.0\n",
      "True YES         240.0           39.0\n",
      "\n",
      "F1: 0.214\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.14\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.219400\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2924.0           49.0\n",
      "True YES         208.0           55.0\n",
      "\n",
      "F1: 0.300\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.21\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.211393\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2902.0           43.0\n",
      "True YES         243.0           48.0\n",
      "\n",
      "F1: 0.251\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.16\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.216754\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2916.0           64.0\n",
      "True YES         196.0           60.0\n",
      "\n",
      "F1: 0.316\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.23\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.210977\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2912.0           50.0\n",
      "True YES         228.0           46.0\n",
      "\n",
      "F1: 0.249\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.17\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.214547\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2917.0           38.0\n",
      "True YES         227.0           54.0\n",
      "\n",
      "F1: 0.290\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.19\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.221068\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2926.0           42.0\n",
      "True YES         210.0           58.0\n",
      "\n",
      "F1: 0.315\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.22\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.215042\n",
      "         Iterations 8\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2911.0           47.0\n",
      "True YES         224.0           54.0\n",
      "\n",
      "F1: 0.285\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.19\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.221130\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2943.0           45.0\n",
      "True YES         203.0           45.0\n",
      "\n",
      "F1: 0.266\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.18\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.220216\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2930.0           36.0\n",
      "True YES         221.0           49.0\n",
      "\n",
      "F1: 0.276\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.18\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.220719\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2926.0           35.0\n",
      "True YES         232.0           43.0\n",
      "\n",
      "F1: 0.244\n",
      "\n",
      "Accuracy on class 0: 0.99\n",
      "Accuracy on class 1: 0.16\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.219935\n",
      "         Iterations 7\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2919.0           51.0\n",
      "True YES         202.0           64.0\n",
      "\n",
      "F1: 0.336\n",
      "\n",
      "Accuracy on class 0: 0.98\n",
      "Accuracy on class 1: 0.24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,d in enumerate(datas):\n",
    "    \n",
    "    for j in range(30):\n",
    "        model_curr,stats_curr,coefs_curr = logreg_and_eval(d,num_rewards=7)\n",
    "        models.append(models)\n",
    "\n",
    "        if ((i == 0 and j == 0)):\n",
    "            stats_0 = stats_curr.copy()\n",
    "            coefs_0 = coefs_curr.copy()\n",
    "        else:\n",
    "            stats_0 = stats_0.append(stats_curr)\n",
    "            coefs_0 = coefs_0.append(coefs_curr)\n",
    "\n",
    "c = np.zeros(90)+70\n",
    "c[30:] = 80\n",
    "c[60:] = 90\n",
    "\n",
    "stats_0.insert(0,'Condition',c)\n",
    "coefs_0.insert(0,'Condition',c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond: 90 u: 0.278 sem: 0.006\n",
      "cond: 80 u: 0.172 sem: 0.003\n",
      "cond: 70 u: 0.104 sem: 0.004\n"
     ]
    }
   ],
   "source": [
    "for cond in [90,80,70]:\n",
    "    s = stats_0[stats_0['Condition']==cond]['F1']\n",
    "    u = s.mean()\n",
    "    sem = s.std() / np.sqrt(s.shape[0])\n",
    "    \n",
    "    print('cond: %.0f u: %.03f sem: %.03f' % (cond,u,sem))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27770091514219503"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'condition':['90-10','90-10','90-10','80-20','80-20','80-20','70-30','70-30','70-30'],\n",
    "    'accuracy': [0.91,0.98,0.18,0.90,0.99,0.10,0.90,0.99,0.05],\n",
    "     'type':['overall','stay','switch','overall','stay','switch','overall','stay','switch']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x106843748>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAF/CAYAAAA/5HFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQJKkGvr/atrx0afJwhAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1107ddd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "master_stats = pd.DataFrame(data=d,index=None)\n",
    "plt.figure(figsize=(3,3))\n",
    "fontsize=14\n",
    "sns.set_style('whitegrid')\n",
    "sns.barplot(x='condition',y='accuracy',data=master_stats,hue='type',\n",
    "            palette=sns.light_palette('black'))\n",
    "plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.yticks([0,0.25,0.5,0.75,1],fontsize=fontsize)\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize,bbox_to_anchor=(1.955,-0.1))\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "#sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAE7CAYAAACmKfb6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHBEJjSFHREL7P+4qY08SOV/vAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1116bf6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "sns.set(font_scale=1.3)\n",
    "sns.set_style('white')\n",
    "sns.barplot(x='Condition',y='F1',data=stats_0)\n",
    "plt.yticks([0,0.1,0.2,0.3])\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>BIC</th>\n",
       "      <th>F1</th>\n",
       "      <th>negative loglikelihood</th>\n",
       "      <th>pseudo-R2</th>\n",
       "      <th>stay</th>\n",
       "      <th>switch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.908531</td>\n",
       "      <td>6068.286383</td>\n",
       "      <td>0.071970</td>\n",
       "      <td>2996.409576</td>\n",
       "      <td>0.654125</td>\n",
       "      <td>0.039501</td>\n",
       "      <td>0.994258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.913384</td>\n",
       "      <td>6153.421895</td>\n",
       "      <td>0.124528</td>\n",
       "      <td>3038.977332</td>\n",
       "      <td>0.649191</td>\n",
       "      <td>0.070815</td>\n",
       "      <td>0.993662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.903864</td>\n",
       "      <td>5994.849594</td>\n",
       "      <td>0.104348</td>\n",
       "      <td>2959.691181</td>\n",
       "      <td>0.658368</td>\n",
       "      <td>0.058366</td>\n",
       "      <td>0.993599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.911704</td>\n",
       "      <td>6066.423085</td>\n",
       "      <td>0.109228</td>\n",
       "      <td>2995.477927</td>\n",
       "      <td>0.654175</td>\n",
       "      <td>0.061181</td>\n",
       "      <td>0.994266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.905544</td>\n",
       "      <td>6007.847589</td>\n",
       "      <td>0.073260</td>\n",
       "      <td>2966.190179</td>\n",
       "      <td>0.657624</td>\n",
       "      <td>0.039293</td>\n",
       "      <td>0.996493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.908344</td>\n",
       "      <td>6056.250598</td>\n",
       "      <td>0.108893</td>\n",
       "      <td>2990.391683</td>\n",
       "      <td>0.654806</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.996497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.911704</td>\n",
       "      <td>6113.847280</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>3019.190024</td>\n",
       "      <td>0.651432</td>\n",
       "      <td>0.068750</td>\n",
       "      <td>0.994669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.907971</td>\n",
       "      <td>6001.260626</td>\n",
       "      <td>0.098720</td>\n",
       "      <td>2962.896697</td>\n",
       "      <td>0.657990</td>\n",
       "      <td>0.056250</td>\n",
       "      <td>0.991798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.914504</td>\n",
       "      <td>6144.778659</td>\n",
       "      <td>0.145522</td>\n",
       "      <td>3034.655714</td>\n",
       "      <td>0.649670</td>\n",
       "      <td>0.084967</td>\n",
       "      <td>0.992242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.906291</td>\n",
       "      <td>6034.969568</td>\n",
       "      <td>0.093863</td>\n",
       "      <td>2979.751168</td>\n",
       "      <td>0.656047</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>0.993417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.906291</td>\n",
       "      <td>5985.964802</td>\n",
       "      <td>0.103571</td>\n",
       "      <td>2955.248785</td>\n",
       "      <td>0.658884</td>\n",
       "      <td>0.057199</td>\n",
       "      <td>0.995052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.906478</td>\n",
       "      <td>6052.961736</td>\n",
       "      <td>0.113274</td>\n",
       "      <td>2988.747252</td>\n",
       "      <td>0.655021</td>\n",
       "      <td>0.064128</td>\n",
       "      <td>0.993001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.907784</td>\n",
       "      <td>6050.194748</td>\n",
       "      <td>0.105072</td>\n",
       "      <td>2987.363758</td>\n",
       "      <td>0.655164</td>\n",
       "      <td>0.060291</td>\n",
       "      <td>0.991386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.905357</td>\n",
       "      <td>6044.973748</td>\n",
       "      <td>0.086486</td>\n",
       "      <td>2984.753258</td>\n",
       "      <td>0.655482</td>\n",
       "      <td>0.047904</td>\n",
       "      <td>0.993822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.906104</td>\n",
       "      <td>5989.083698</td>\n",
       "      <td>0.115993</td>\n",
       "      <td>2956.808233</td>\n",
       "      <td>0.658701</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.993816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.910398</td>\n",
       "      <td>6087.048329</td>\n",
       "      <td>0.136691</td>\n",
       "      <td>3005.790549</td>\n",
       "      <td>0.653052</td>\n",
       "      <td>0.079332</td>\n",
       "      <td>0.992005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.907971</td>\n",
       "      <td>6058.336887</td>\n",
       "      <td>0.130511</td>\n",
       "      <td>2991.434828</td>\n",
       "      <td>0.654694</td>\n",
       "      <td>0.076446</td>\n",
       "      <td>0.990560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.905731</td>\n",
       "      <td>6047.584776</td>\n",
       "      <td>0.124783</td>\n",
       "      <td>2986.058773</td>\n",
       "      <td>0.655331</td>\n",
       "      <td>0.072435</td>\n",
       "      <td>0.990947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.906291</td>\n",
       "      <td>6023.613607</td>\n",
       "      <td>0.093863</td>\n",
       "      <td>2974.073188</td>\n",
       "      <td>0.656710</td>\n",
       "      <td>0.052104</td>\n",
       "      <td>0.994030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.903677</td>\n",
       "      <td>6009.180185</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>2966.856477</td>\n",
       "      <td>0.657524</td>\n",
       "      <td>0.058708</td>\n",
       "      <td>0.992778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.905544</td>\n",
       "      <td>5970.639172</td>\n",
       "      <td>0.073260</td>\n",
       "      <td>2947.585970</td>\n",
       "      <td>0.659767</td>\n",
       "      <td>0.039293</td>\n",
       "      <td>0.996493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.907598</td>\n",
       "      <td>6041.573831</td>\n",
       "      <td>0.104882</td>\n",
       "      <td>2983.053300</td>\n",
       "      <td>0.655665</td>\n",
       "      <td>0.058704</td>\n",
       "      <td>0.993831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.910211</td>\n",
       "      <td>6077.757727</td>\n",
       "      <td>0.110906</td>\n",
       "      <td>3001.145248</td>\n",
       "      <td>0.653572</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>0.994663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.914131</td>\n",
       "      <td>6170.090204</td>\n",
       "      <td>0.135338</td>\n",
       "      <td>3047.311486</td>\n",
       "      <td>0.648252</td>\n",
       "      <td>0.079646</td>\n",
       "      <td>0.991030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>6076.992367</td>\n",
       "      <td>0.093110</td>\n",
       "      <td>3000.762568</td>\n",
       "      <td>0.653637</td>\n",
       "      <td>0.052083</td>\n",
       "      <td>0.993439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.910584</td>\n",
       "      <td>6120.889149</td>\n",
       "      <td>0.055227</td>\n",
       "      <td>3022.710959</td>\n",
       "      <td>0.651099</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.993667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.907224</td>\n",
       "      <td>5994.875104</td>\n",
       "      <td>0.091408</td>\n",
       "      <td>2959.703936</td>\n",
       "      <td>0.658348</td>\n",
       "      <td>0.050201</td>\n",
       "      <td>0.995061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.904984</td>\n",
       "      <td>6068.180232</td>\n",
       "      <td>0.092692</td>\n",
       "      <td>2996.356500</td>\n",
       "      <td>0.654112</td>\n",
       "      <td>0.051181</td>\n",
       "      <td>0.994432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.912264</td>\n",
       "      <td>6112.732033</td>\n",
       "      <td>0.089147</td>\n",
       "      <td>3018.632401</td>\n",
       "      <td>0.651542</td>\n",
       "      <td>0.049676</td>\n",
       "      <td>0.993870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.908904</td>\n",
       "      <td>6032.396889</td>\n",
       "      <td>0.119134</td>\n",
       "      <td>2978.464829</td>\n",
       "      <td>0.656211</td>\n",
       "      <td>0.067347</td>\n",
       "      <td>0.993631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Condition  Accuracy          BIC        F1  negative loglikelihood  \\\n",
       "0       70.0  0.908531  6068.286383  0.071970             2996.409576   \n",
       "0       70.0  0.913384  6153.421895  0.124528             3038.977332   \n",
       "0       70.0  0.903864  5994.849594  0.104348             2959.691181   \n",
       "0       70.0  0.911704  6066.423085  0.109228             2995.477927   \n",
       "0       70.0  0.905544  6007.847589  0.073260             2966.190179   \n",
       "0       70.0  0.908344  6056.250598  0.108893             2990.391683   \n",
       "0       70.0  0.911704  6113.847280  0.122449             3019.190024   \n",
       "0       70.0  0.907971  6001.260626  0.098720             2962.896697   \n",
       "0       70.0  0.914504  6144.778659  0.145522             3034.655714   \n",
       "0       70.0  0.906291  6034.969568  0.093863             2979.751168   \n",
       "0       70.0  0.906291  5985.964802  0.103571             2955.248785   \n",
       "0       70.0  0.906478  6052.961736  0.113274             2988.747252   \n",
       "0       70.0  0.907784  6050.194748  0.105072             2987.363758   \n",
       "0       70.0  0.905357  6044.973748  0.086486             2984.753258   \n",
       "0       70.0  0.906104  5989.083698  0.115993             2956.808233   \n",
       "0       70.0  0.910398  6087.048329  0.136691             3005.790549   \n",
       "0       70.0  0.907971  6058.336887  0.130511             2991.434828   \n",
       "0       70.0  0.905731  6047.584776  0.124783             2986.058773   \n",
       "0       70.0  0.906291  6023.613607  0.093863             2974.073188   \n",
       "0       70.0  0.903677  6009.180185  0.104167             2966.856477   \n",
       "0       70.0  0.905544  5970.639172  0.073260             2947.585970   \n",
       "0       70.0  0.907598  6041.573831  0.104882             2983.053300   \n",
       "0       70.0  0.910211  6077.757727  0.110906             3001.145248   \n",
       "0       70.0  0.914131  6170.090204  0.135338             3047.311486   \n",
       "0       70.0  0.909091  6076.992367  0.093110             3000.762568   \n",
       "0       70.0  0.910584  6120.889149  0.055227             3022.710959   \n",
       "0       70.0  0.907224  5994.875104  0.091408             2959.703936   \n",
       "0       70.0  0.904984  6068.180232  0.092692             2996.356500   \n",
       "0       70.0  0.912264  6112.732033  0.089147             3018.632401   \n",
       "0       70.0  0.908904  6032.396889  0.119134             2978.464829   \n",
       "\n",
       "   pseudo-R2      stay    switch  \n",
       "0   0.654125  0.039501  0.994258  \n",
       "0   0.649191  0.070815  0.993662  \n",
       "0   0.658368  0.058366  0.993599  \n",
       "0   0.654175  0.061181  0.994266  \n",
       "0   0.657624  0.039293  0.996493  \n",
       "0   0.654806  0.059524  0.996497  \n",
       "0   0.651432  0.068750  0.994669  \n",
       "0   0.657990  0.056250  0.991798  \n",
       "0   0.649670  0.084967  0.992242  \n",
       "0   0.656047  0.052419  0.993417  \n",
       "0   0.658884  0.057199  0.995052  \n",
       "0   0.655021  0.064128  0.993001  \n",
       "0   0.655164  0.060291  0.991386  \n",
       "0   0.655482  0.047904  0.993822  \n",
       "0   0.658701  0.065217  0.993816  \n",
       "0   0.653052  0.079332  0.992005  \n",
       "0   0.654694  0.076446  0.990560  \n",
       "0   0.655331  0.072435  0.990947  \n",
       "0   0.656710  0.052104  0.994030  \n",
       "0   0.657524  0.058708  0.992778  \n",
       "0   0.659767  0.039293  0.996493  \n",
       "0   0.655665  0.058704  0.993831  \n",
       "0   0.653572  0.061856  0.994663  \n",
       "0   0.648252  0.079646  0.991030  \n",
       "0   0.653637  0.052083  0.993439  \n",
       "0   0.651099  0.030303  0.993667  \n",
       "0   0.658348  0.050201  0.995061  \n",
       "0   0.654112  0.051181  0.994432  \n",
       "0   0.651542  0.049676  0.993870  \n",
       "0   0.656211  0.067347  0.993631  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_0[stats_0['Condition']==70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing beta coefficients across conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASsAAAEvCAYAAAAdNeeiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ned8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11111b2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coefs = coefs_0\n",
    "sns.set_style('white')\n",
    "fontsize = 15\n",
    "plt.figure(figsize=(3,3))\n",
    "\n",
    "plt.errorbar(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 90].iloc[:,1:].mean(),\n",
    "             yerr= coefs[coefs['Condition'] == 90].iloc[:,1:].mean() / np.sqrt(np.sum(coefs['Condition'] == 90)),\n",
    "             color='red',alpha=0.5,linewidth=5)\n",
    "plt.errorbar(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 80].iloc[:,1:].mean(),\n",
    "             yerr= coefs[coefs['Condition'] == 90].iloc[:,1:].mean() / np.sqrt(np.sum(coefs['Condition'] == 90)),\n",
    "             color='green',alpha=0.5,linewidth=5)\n",
    "plt.errorbar(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 70].iloc[:,1:].mean(),\n",
    "             yerr= coefs[coefs['Condition'] == 90].iloc[:,1:].mean() / np.sqrt(np.sum(coefs['Condition'] == 90)),\n",
    "             color='navy',alpha=0.5,linewidth=5)\n",
    "\n",
    "plt.scatter(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 90].iloc[:,1:].mean(),\n",
    "            color='red',label='90-10',s=100)\n",
    "plt.scatter(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 80].iloc[:,1:].mean(),\n",
    "            color='green',label='80-20',s=100)\n",
    "plt.scatter(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 70].iloc[:,1:].mean(),\n",
    "            color='navy',label='70-30',s=100)\n",
    "\n",
    "plt.legend(loc='upper left',fontsize=fontsize)\n",
    "#plt.xticks(np.arange(coefs.shape[1]),coefs.columns.values[1:],rotation='vertical')\n",
    "plt.xlabel('')\n",
    "plt.yticks([0,1,2],fontsize=fontsize)\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.ylabel('coefficient value',fontsize=fontsize)\n",
    "plt.xlim(-0.5,8.5)\n",
    "plt.title('Regression Coefficients',fontsize=fontsize)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coefficients by mouse (For 80-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mice = np.unique(data_80['Mouse ID'].values)\n",
    "\n",
    "for i,mouse in enumerate(mice):\n",
    "    d = data_80[data_80['Mouse ID'] == mouse]\n",
    "    \n",
    "    for j in range(30):\n",
    "        model_curr,stats_curr,coefs_curr = logreg_and_eval(d,num_rewards=7)\n",
    "        models.append(models)\n",
    "        \n",
    "        stats_curr.insert(0,'Mouse ID',mouse)\n",
    "        coefs_curr.insert(0,'Mouse ID',mouse)\n",
    "\n",
    "        if ((i == 0 and j == 0)):\n",
    "            stats_0_m = stats_curr.copy()\n",
    "            coefs_0_m = coefs_curr.copy()\n",
    "        else:\n",
    "            stats_0_m = stats_0_m.append(stats_curr)\n",
    "            coefs_0_m = coefs_0_m.append(coefs_curr,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coefs_0_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coefs = coefs_0_m.copy()\n",
    "\n",
    "sns.set_style('white')\n",
    "fontsize = 15\n",
    "colors = sns.color_palette(palette='husl',n_colors=mice.shape[0])\n",
    "plt.figure(figsize=(3,3))\n",
    "\n",
    "for mouse,color in zip(mice,colors):\n",
    "    if mouse != 'K1':\n",
    "        curr_coefs = coefs[coefs['Mouse ID']==mouse]\n",
    "\n",
    "        #print(curr_coefs.head(5))\n",
    "        plt.errorbar(np.arange(coefs.shape[1]-1),curr_coefs.iloc[:,1:].mean(),\n",
    "                     yerr= curr_coefs.iloc[:,1:].mean() / np.sqrt(curr_coefs.shape[0]),\n",
    "                     color=color,alpha=0.8,linewidth=2)\n",
    "\n",
    "        plt.scatter(np.arange(coefs.shape[1]-1),curr_coefs.iloc[:,1:].mean(),\n",
    "                    color=color,label=mouse,s=50,alpha=0.3)\n",
    "\n",
    "#plt.legend(bbox_to_anchor=(2,1),fontsize=fontsize)\n",
    "#plt.xticks(np.arange(coefs.shape[1]),coefs.columns.values[1:],rotation='vertical')\n",
    "plt.xlabel('')\n",
    "plt.yticks([0,1,2,3],fontsize=fontsize)\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.ylabel('coefficient value',fontsize=fontsize)\n",
    "plt.xlim(-0.5,8.5)\n",
    "plt.title('Regression Coefficients',fontsize=fontsize)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test number of parameters / model flexibility vs BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = data_80.copy()\n",
    "stats = pd.DataFrame(columns=['Accuracy','BIC','negative loglikelihood','pseudo-R2','No. parameters','F1'])\n",
    "\n",
    "for i,n in enumerate(np.arange(10,0,-1)):\n",
    "    \n",
    "    for j in enumerate(range(30)):\n",
    "        model_curr,stats_curr,coefs_curr = logreg_and_eval(d,num_rewards=n)\n",
    "        models.append(models)\n",
    "        stats_curr['No. parameters'] = n\n",
    "        stats = stats.append(stats_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "sns.factorplot(x='No. parameters',y='BIC',data=stats,color='black')\n",
    "plt.title('BIC vs model flexibility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BIC = np.zeros((10,10))\n",
    "F1 = np.zeros((10,10))\n",
    "R2 = np.zeros((10,10))\n",
    "acc = np.zeros((10,10))\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if not ((j==0) & (i==0)):\n",
    "            model_curr,stats_curr,coefs_curr = logreg_and_eval_withports(d,num_rewards=i,num_ports=j)\n",
    "            BIC[i,j] = stats_curr['BIC'].values\n",
    "            F1[i,j] = stats_curr['F1'].values\n",
    "            R2[i,j] = stats_curr['pseudo-R2'].values\n",
    "            acc[i,j] = stats_curr['Accuracy'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(F1)\n",
    "plt.xlabel('# previous decisions',fontsize=15)\n",
    "plt.ylabel('# previous reward outcomes',fontsize=15)\n",
    "plt.title('F1',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training / testing on different conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train on 90-10, test on 80-20 and 70-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,d in enumerate([data_90,data_80,data_70]):\n",
    "    \n",
    "    if i == 0:\n",
    "        model,stats,coefs = logreg_and_eval(d)\n",
    "    else:\n",
    "        model,stats_curr,coefs_curr = logreg_and_eval(data_90,test_data = d)\n",
    "        \n",
    "        stats = stats.append(stats_curr)\n",
    "        coefs = coefs.append(coefs_curr)\n",
    "\n",
    "stats_90 = stats.drop(['BIC','negative loglikelihood','pseudo-R2'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats_90['Testing Condition'] = [90,80,70]\n",
    "stats_90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is interesting - accuracy stays pretty much the same across conditions, but F1 goes way down. And if we take a look at the confusion tables above, we can see it is because the accuracy on the switches went down (and accuracy on stays went up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train on 80-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,d in enumerate([data_80,data_90,data_70]):\n",
    "    \n",
    "    if i == 0:\n",
    "        model,stats,coefs = logreg_and_eval(d)\n",
    "    else:\n",
    "        model,stats_curr,coefs_curr = logreg_and_eval(data_80,test_data = d)\n",
    "        \n",
    "        stats = stats.append(stats_curr)\n",
    "        coefs = coefs.append(coefs_curr)\n",
    "\n",
    "stats_80 = stats.drop(['BIC','negative loglikelihood','pseudo-R2'],axis=1)\n",
    "\n",
    "stats_80['Testing Condition'] = [80,90,70]\n",
    "stats_80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train on 70-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,d in enumerate([data_70,data_80,data_90]):\n",
    "    \n",
    "    if i == 0:\n",
    "        model,stats,coefs = logreg_and_eval(d)\n",
    "    else:\n",
    "        model,stats_curr,coefs_curr = logreg_and_eval(data_70,test_data = d)\n",
    "        \n",
    "        stats = stats.append(stats_curr)\n",
    "        coefs = coefs.append(coefs_curr)\n",
    "\n",
    "stats_70 = stats.drop(['BIC','negative loglikelihood','pseudo-R2'],axis=1)\n",
    "\n",
    "stats_70['Testing Condition'] = [70,80,90]\n",
    "stats_70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1s = np.vstack((stats_90['F1'].values,\n",
    "               stats_80['F1'].values[[1,0,2]],\n",
    "               stats_70['F1'].values[[2,1,0]]))\n",
    "sns.heatmap(f1s,vmin=0,vmax=0.4)\n",
    "plt.xticks([0.5,1.5,2.5],['90','80','70'])\n",
    "plt.yticks([0.5,1.5,2.5],['70','80','90'])\n",
    "plt.ylabel('Testing Condition')\n",
    "plt.xlabel('Training Condition')\n",
    "plt.title('F1 scores when trained & tested on different conditions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if this is right, it means that the rules are different - what predicts a switch in 90-10 does not predict a switch in 80-20. But since most of the trials follow the last one, the accuracy doesn't drop very much. So it appears to be working fine, even though it is not. \n",
    "\n",
    "Can the difference be explained in the small differences in beta coefficient values? It must be ... what else is there? They seem similar enough that I'm surprised it makes such a difference. \n",
    "\n",
    "Let's go on to train and test on separate mice:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test on separate mice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's take a quick look at the mice's performances - specifically just at p(choose high P port):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_90.insert(0,'Condition',0.9)\n",
    "data_80.insert(0,'Condition',0.8)\n",
    "data_70.insert(0,'Condition',0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = data_90.append(data_80)\n",
    "all_data = all_data.append(data_70)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x='Condition',y='Higher p port',hue='Mouse ID',data = all_data,legend=False,size=5,aspect=1.7)\n",
    "plt.legend(bbox_to_anchor=(1.2,1))\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('Condition',fontsize=20)\n",
    "plt.ylabel('rate higher prob port chosen',fontsize=20)\n",
    "plt.title('Average p(choose better port) for each mouse across conditions',fontsize=20,x=0.5,y=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so based on this. Let's start with 80-20, and do a few different comparisons. \n",
    "\n",
    "1. Start by training with harry, and testing on all the others. \n",
    "2. Then try training on volde, testing on all others. \n",
    "3. Finally train on someone in the middle - like Tom or q45, and test on others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Loop through mice\n",
    "'''\n",
    "mice = np.unique(data_80['Mouse ID'].values)\n",
    "\n",
    "stats = pd.DataFrame(columns=['Accuracy','F1','Training Mouse','Testing Mouse'])\n",
    "test_mice = []\n",
    "train_mice = []\n",
    "\n",
    "for mouse_train in mice:\n",
    "\n",
    "    d_train = data_80[data_80['Mouse ID'] == mouse_train].copy()\n",
    "\n",
    "    for i,mouse_test in enumerate(mice):\n",
    "        d_test = data_80[data_80['Mouse ID'] == mouse_test].copy()\n",
    "\n",
    "        if i == 0:\n",
    "            model,stats_curr,coefs = logreg_and_eval(d_train,test_data = d_test)\n",
    "            stats_curr = stats_curr.drop(['BIC','negative loglikelihood','pseudo-R2'],axis=1)\n",
    "            stats = stats.append(stats_curr)\n",
    "        else:\n",
    "            model,stats_curr,coefs_curr = logreg_and_eval(d_train,test_data = d_test)\n",
    "            stats_curr = stats_curr.drop(['BIC','negative loglikelihood','pseudo-R2'],axis=1)\n",
    "            stats= stats.append(stats_curr)\n",
    "            coefs = coefs.append(coefs_curr)\n",
    "\n",
    "        test_mice.append(mouse_test)\n",
    "        train_mice.append(mouse_train)\n",
    "\n",
    "stats['Testing Mouse'] = test_mice\n",
    "stats['Training Mouse'] = train_mice\n",
    "acc_matrix = np.reshape(stats['Accuracy'].values,(len(mice),-1)).T\n",
    "F1_matrix = np.reshape(stats['F1'].values,(len(mice),-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fontsize=20\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "sns.set(font_scale=1.8)\n",
    "sns.heatmap(acc_matrix)\n",
    "plt.xticks(np.arange(11)+0.5,rotation='horizontal',fontsize=fontsize)\n",
    "plt.yticks(np.arange(11)+0.5,rotation='horizontal',fontsize=fontsize)\n",
    "plt.xlabel('Training Mouse',fontsize=fontsize)\n",
    "plt.ylabel('Testing Mouse',fontsize=fontsize)\n",
    "plt.title('Accuracy',fontsize=fontsize)\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.heatmap(F1_matrix)\n",
    "plt.xticks(np.arange(11)+0.5,rotation='horizontal',fontsize=fontsize)\n",
    "plt.yticks(np.arange(11)+0.5,rotation='horizontal',fontsize=fontsize)\n",
    "plt.xlabel('Training Mouse',fontsize=fontsize)\n",
    "plt.ylabel('Testing Mouse',fontsize=20)\n",
    "plt.title('F1',fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_80.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u_switch = np.zeros(len(mice))\n",
    "u_acc = np.zeros(len(mice))\n",
    "\n",
    "for i,mouse in enumerate(mice):\n",
    "    u_switch[i] = data_80[data_80['Mouse ID']== mouse]['Switch'].mean()\n",
    "    u_acc[i] = data_80[data_80['Mouse ID']== mouse]['Higher p port'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette('hls',n_colors=len(mice))\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.suptitle('Higher switch rates anti-correlate with accuracy on test mice',fontsize=20,x=0.5,y=1.1)\n",
    "plt.subplot(121)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_switch[i],acc_matrix[i,:].mean(),label=mouse,c=colors[i],s=50)\n",
    "#plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean switch rate')\n",
    "plt.ylabel('mean accuracy when used as test mouse')\n",
    "plt.ylim(0.75,1)\n",
    "plt.xlim(0,0.22)\n",
    "plt.title('Testing accuracy vs mean switch rate')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_switch[i],acc_matrix[:,i].mean(),label=mouse,c=colors[i],s=50)\n",
    "plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean switch rate')\n",
    "plt.ylabel('mean accuracy when used as train mouse')\n",
    "plt.ylim(0.75,1)\n",
    "plt.xlim(0,0.22)\n",
    "plt.title('Training accuracy vs mean switch rate')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette('hls',n_colors=len(mice))\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.suptitle('Higher switch rates correlate with F1 score on test mice',fontsize=20,x=0.5,y=1.1)\n",
    "plt.subplot(121)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_switch[i],F1_matrix[i,:].mean(),label=mouse,c=colors[i],s=50)\n",
    "#plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean switch rate')\n",
    "plt.ylabel('mean F1 when used as test mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Testing F1 vs mean switch rate')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_switch[i],F1_matrix[:,i].mean(),label=mouse,c=colors[i],s=50)\n",
    "plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean switch rate')\n",
    "plt.ylabel('mean F1 when used as train mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Training F1 vs mean switch rate')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette('hls',n_colors=len(mice))\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.suptitle('Better behavior correlates with accuracy score on test mice',fontsize=20,x=0.5,y=1.1)\n",
    "plt.subplot(121)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_acc[i],acc_matrix[i,:].mean(),label=mouse,c=colors[i],s=50)\n",
    "#plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean p(high p port)')\n",
    "plt.ylabel('mean acc when used as test mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Testing Acc vs mean p(high p port)')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_acc[i],acc_matrix[:,i].mean(),label=mouse,c=colors[i],s=50)\n",
    "plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean p(high p port)')\n",
    "plt.ylabel('mean acc when used as train mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Training acc vs mean p(high p port)')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette('hls',n_colors=len(mice))\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.suptitle('p(high p port) vs F1 score',fontsize=20,x=0.5,y=1.1)\n",
    "plt.subplot(121)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_acc[i],F1_matrix[i,:].mean(),label=mouse,c=colors[i],s=50)\n",
    "#plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('p(high p port)')\n",
    "plt.ylabel('mean F1 when used as test mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Testing F1 vs mean switch rate')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_acc[i],F1_matrix[:,i].mean(),label=mouse,c=colors[i],s=50)\n",
    "plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('p(high p port)')\n",
    "plt.ylabel('mean F1 when used as train mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Training F1 vs mean switch rate')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2,interaction_only=True,include_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logreg_and_eval_withports_and_interactions(data,num_rewards=10,num_ports=1,test_data=False):\n",
    "    '''\n",
    "    Perform Logistic Regression on a pandas dataframe of trials (from feature matrix) with interactions\n",
    "    \n",
    "    Inputs:\n",
    "        - data: pandas dataframe of trials (from feature matrix)\n",
    "    Outputs:\n",
    "        - logreg: trained logistic regression model (from sklearn)\n",
    "        - stats:  pandas dataframe with F1, pseudo-R2, and BIC scores from model\n",
    "        - coeffs: beta coefficients from logreg\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    from statsmodels.discrete.discrete_model import Logit\n",
    "    \n",
    "    port_features = []\n",
    "    reward_features = []\n",
    "\n",
    "    #change right port to -1 instead of 0\n",
    "    for col in data:\n",
    "        if '_Port' in col:\n",
    "            data.loc[data[col] == 0,col] = -1\n",
    "            port_features.append(col)\n",
    "        elif '_Reward' in col:\n",
    "            reward_features.append(col)\n",
    "\n",
    "    #create new feature matrix\n",
    "    d = data.copy()\n",
    "    for i in range(len(port_features)):\n",
    "        d[reward_features[i]] = d[reward_features[i]].values*d[port_features[i]].values\n",
    "    \n",
    "    \n",
    "    #determine the features\n",
    "    features = reward_features.copy()\n",
    "    if num_rewards == 0:\n",
    "        features = port_features[-1*num_ports:]\n",
    "    elif num_ports == 0:\n",
    "        features = features[-1*num_rewards:] #only take the num of rewards specificied in the function\n",
    "    else:\n",
    "        features = features[-1*num_rewards:]\n",
    "        features = np.append(features,port_features[-1*num_ports:])\n",
    "    \n",
    "    print(features)\n",
    "    features = np.append(features,'Decision') #finally append the decision so we can take it to predict later\n",
    "    \n",
    "    \n",
    "    \n",
    "    #final version of data\n",
    "    d = d[features].copy() #this now just has the features we want and the decision we want to predict\n",
    "    \n",
    "    #do the same thing for the test data if it exists!\n",
    "    if test_data is not False:\n",
    "        for col in test_data:\n",
    "            if '_Port' in col:\n",
    "                test_data.loc[test_data[col] == 0,col] = -1\n",
    "\n",
    "        #create new feature matrix\n",
    "        data_test_new = test_data.copy()\n",
    "        for i in range(len(port_features)):\n",
    "            data_test_new[reward_features[i]] = test_data[reward_features[i]].values*test_data[port_features[i]].values\n",
    "        \n",
    "        d_test = data_test_new[features].copy()\n",
    "    \n",
    "    \n",
    "        #set training and testing sets now\n",
    "        x_train = d.iloc[:,:-1].values\n",
    "        y_train = d.iloc[:,-1].values\n",
    "        x_test = d_test.iloc[:,:-1].values\n",
    "        y_test = d_test.iloc[:,-1].values\n",
    "        \n",
    "        prev_port_test = test_data['1_Port'].values\n",
    "        prev_port_test[prev_port_test==-1] = 0\n",
    "    \n",
    "    #if there is no test data, then split up the data into training and testing\n",
    "    else:\n",
    "        #extract features and decisions\n",
    "        x = d.iloc[:,:-1].values\n",
    "        y = d.iloc[:,-1].values\n",
    "\n",
    "        #split into training and testing\n",
    "        n_trials = x.shape[0]\n",
    "        shuf_inds = np.random.permutation(n_trials)\n",
    "        split_ind = int(n_trials*0.7)\n",
    "\n",
    "        x_train = x[shuf_inds[:split_ind],:]\n",
    "        y_train = y[shuf_inds[:split_ind]]\n",
    "\n",
    "        x_test = x[shuf_inds[split_ind:],:]\n",
    "        y_test = y[shuf_inds[split_ind:]]\n",
    "        \n",
    "        #extract previous port decision for test set\n",
    "        #these will be used to calculate switches on the test predictions\n",
    "        prev_port_test = data['1_Port'].values[shuf_inds[split_ind:]]\n",
    "        prev_port_test[prev_port_test==-1] = 0\n",
    "    \n",
    "    '''\n",
    "    Modeling\n",
    "    '''\n",
    "    \n",
    "    #create interaction terms\n",
    "    poly = PolynomialFeatures(degree=5,interaction_only=True,include_bias=True)\n",
    "    x_train = poly.fit_transform(x_train)\n",
    "    x_test = poly.fit_transform(x_test)\n",
    "    print('x_train shape: %.0f' % x_train.shape[1])\n",
    "    \n",
    "    \n",
    "    #fit logistic regression\n",
    "    logreg = sklearn.linear_model.LogisticRegressionCV()\n",
    "    logreg.fit(x_train,y_train)\n",
    "    \n",
    "    #predict on testing set\n",
    "    y_predict = logreg.predict(x_test)\n",
    "    y_predict_proba = logreg.predict_proba(x_test)\n",
    "    \n",
    "    #model accuracy\n",
    "    score = logreg.score(x_test,y_test)\n",
    "    \n",
    "    #calculating pseudo-R2 and BIC from statsmodel OLS\n",
    "    model = Logit(y_train,x_train)\n",
    "    rslt  = model.fit()\n",
    "\n",
    "    #switches\n",
    "    y_test_switch = np.abs(y_test - prev_port_test)\n",
    "    y_predict_switch = np.abs(y_predict - prev_port_test)\n",
    "    acc_pos,acc_neg,F1=sf.score_both_and_confuse(y_predict_switch,y_test_switch,confusion=False,disp=True)\n",
    "    \n",
    "    #extract coefficients\n",
    "    coefs = logreg.coef_ #retrieve coefs\n",
    "    coefs = np.append(coefs[0],logreg.intercept_) #add bias coef\n",
    "    \n",
    "    #create stats database to return\n",
    "    d_ = {'pseudo-R2':rslt.prsquared,'stay':acc_pos,'switch':acc_neg,'Accuracy':score,'BIC':rslt.bic,'negative loglikelihood':-1*rslt.llf}\n",
    "    stats = pd.DataFrame(data=d_,index=[0])\n",
    "    features = features[:-1]\n",
    "    features = np.append(features,'Bias')\n",
    "    \n",
    "    #coefs = pd.DataFrame(data=coefs.reshape(1,-1),columns=poly.get_feature_names())\n",
    "    return logreg,stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model,stats_curr = logreg_and_eval_withports_and_interactions(data_80,num_rewards = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.coef_.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(model.coef_.shape[1]),model.coef_)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:CaGRIN]",
   "language": "python",
   "name": "conda-env-CaGRIN-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
